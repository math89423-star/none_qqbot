import urllib.parse
import random
import aiohttp
import time
import threading
import math
import logging
import io
from PIL import Image
from http import HTTPStatus
from datetime import datetime, timedelta, timezone
from .error_utils import PixivAPIError
from ..config.config import (
    PIXIV_COOKIE, 
    PROXY, 
    PROXY_URL, 
    USE_PROXY, 
    EXCLUDE_DURATION
)

# åˆ›å»ºæ—¥å¿—
logger = logging.getLogger()
logging.basicConfig(level = logging.INFO,format = '%(asctime)s - %(name)s - %(levelname)s - %(message)s')

RECENT_IMAGES = {}
# æ·»åŠ å…¨å±€é”
RECENT_IMAGES_LOCK = threading.Lock()

# æ ¸å¿ƒè¾…åŠ©å‡½æ•°
def _is_r18_request(tags: list) -> bool:
    """æ£€æŸ¥æ˜¯å¦æ˜ç¡®è¯·æ±‚R-18å†…å®¹"""
    return any(tag.lower() in ["r-18", "r18", "r-18g", "r18g"] for tag in tags)

def _build_search_strategies() -> list:
    """æ„å»ºä¸‰é˜¶æ®µæœç´¢ç­–ç•¥é…ç½®"""
    return [
        {
            "name": "ç²¾å‡†æ¨¡å¼(90å¤©+é«˜æ”¶è—)",
            "params": {
                "scd": (datetime.now(timezone.utc) - timedelta(days=90)).strftime("%Y-%m-%d"),
                "ecd": datetime.now(timezone.utc).strftime("%Y-%m-%d"),
                "blt": "500"
            },
            "page": 1,  # ç²¾å‡†æ¨¡å¼å›ºå®šç¬¬ä¸€é¡µ
            "mode": "s_tag"  # æ˜ç¡®æœç´¢æ¨¡å¼
        },
        {
            "name": "å®½æ¾æ¨¡å¼(360å¤©+ä¸­æ”¶è—)",
            "params": {
                "scd": (datetime.now(timezone.utc) - timedelta(days=360)).strftime("%Y-%m-%d"),
                "ecd": datetime.now(timezone.utc).strftime("%Y-%m-%d"),
                "blt": "100"
            },
            "page_range": (1, 3),  # 1-3é¡µ
            "mode": "s_tag"
        },
        {
            "name": "å…¨ç«™æ¨¡å¼(æ— é™åˆ¶)",
            "params": {},
            "page_range": (1, 5),  # 1-5é¡µ
            "mode": "s_tag"
        }
    ]

def _build_pixiv_headers(tags: list) -> dict:
    search_tag = " ".join(tags)
    """æ„å»ºPixivè¯·æ±‚å¤´"""
    return {
        "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36",
        "Referer": f"https://www.pixiv.net/tags/{urllib.parse.quote(search_tag)}/artworks",
        "Accept-Language": "zh-CN,zh;q=0.9,en;q=0.8",
        "Cookie": PIXIV_COOKIE,
        "Sec-Fetch-Dest": "empty",
        "Sec-Fetch-Mode": "cors",
        "Sec-Fetch-Site": "same-origin",
        "X-Requested-With": "XMLHttpRequest"
    }

async def _execute_search_strategy(
    search_tag: str,
    encoded_tag: str,
    strategy: dict,
) -> list:
    """æ‰§è¡Œå•æ¬¡æœç´¢ç­–ç•¥å¹¶è¿”å›åŸå§‹ç»“æœåˆ—è¡¨"""
    headers = _build_pixiv_headers(search_tag)
    proxy = PROXY if USE_PROXY else None
       # ä»ç­–ç•¥è·å–é¡µç  (ç²¾å‡†æ¨¡å¼å›ºå®šç¬¬ä¸€é¡µ)
    if "page" in strategy:
        page = strategy["page"]
    else:
        page_range = strategy.get("page_range", (1, 3))
        page = random.randint(*page_range)
    # ä»ç­–ç•¥è·å–æœç´¢æ¨¡å¼
    search_mode = strategy.get("mode", "s_tag")
    params = {
        "word": search_tag,
        "order": "popular_d",  # æŒ‰å—æ¬¢è¿åº¦æ’åº
        "mode": search_mode,
        "p": page,
        "s_mode": "s_tag",     # æ ‡ç­¾å®Œå…¨åŒ¹é…
        "type": "all",
        "lang": "zh",
        **strategy["params"]
    }
    # æ·»åŠ è°ƒè¯•æ—¥å¿—
    logger.debug(f"è¯·æ±‚ç­–ç•¥: {strategy['name']}, é¡µç : {page}, å‚æ•°: {params}")
    async with aiohttp.ClientSession() as session, \
        session.get(
            f"https://www.pixiv.net/ajax/search/artworks/{encoded_tag}",
            headers=headers,
            params=params,
            proxy=proxy,
            timeout=30
        ) as response:
            if response.status != HTTPStatus.OK:
                raise PixivAPIError(
                    error_type = "api_failure",
                    strategy_name = strategy['name'],
                    details={"status": response.status}
                )
            data = await response.json()
            if not data.get("body") or not data["body"].get("illustManga", {}).get("data"):
                raise PixivAPIError(
                    error_type = "empty_data",
                    strategy_name = strategy['name'],
                    details={"status": response.status}
                )
            return data["body"]["illustManga"]["data"]

def _extract_tag_names(item: dict) -> list:
    """æå–ä½œå“æ ‡ç­¾"""
    tags_info = item.get("tags", [])
    if isinstance(tags_info, dict):
        tags_info = tags_info.get("tags", [])
    return [
        tag.get("tag", "").lower()
        for tag in tags_info
        if isinstance(tag, dict)
    ]

def _is_r18_content(tag_names: list) -> bool:
    """æ£€æŸ¥R-18å†…å®¹"""
    return any("r-18" in tag or "r18" in tag for tag in tag_names)

def _calculate_quality_scores(
    items: list,
    current_time: datetime
) -> list:
    """è®¡ç®—ä½œå“è´¨é‡è¯„åˆ†ï¼ˆä¼˜åŒ–ç‰ˆï¼šç»¼åˆè€ƒè™‘ç»å¯¹æ•°é‡ã€äº’åŠ¨æ¯”ç‡å’Œæ–°é²œåº¦ï¼‰"""
    scored_items = []
    for item in items:
        # åŸºç¡€æŒ‡æ ‡
        bookmark_count = item.get("bookmarkCount", 0)  # æ”¶è—æ•°
        like_count = item.get("likeCount", 0)          # ç‚¹èµæ•°
        view_count = max(1, item.get("viewCount", 1))  # æµè§ˆé‡ï¼Œè‡³å°‘ä¸º1
        # 1. è®¡ç®—åŸºç¡€è´¨é‡å¾—åˆ†
        # 1.1 ç»å¯¹äº’åŠ¨åˆ†ï¼ˆæ”¶è—æƒé‡æ›´é«˜ï¼Œåæ˜ Pixivå¹³å°ç‰¹æ€§ï¼‰
        absolute_score = bookmark_count * 5 + like_count * 3
        # 1.2 æ¯”ç‡åˆ†ï¼ˆé«˜è´¨é‡ä½æ›å…‰ä½œå“çš„è¡¥å¿æœºåˆ¶ï¼‰
        bookmark_ratio = bookmark_count / view_count
        like_ratio = like_count / view_count
        # æ¯”ç‡åˆ†ï¼šå°†äº’åŠ¨æ¯”ç‡è½¬æ¢ä¸ºåŠ åˆ†é¡¹ï¼ˆæ”¶è—ç‡5%+å’Œç‚¹èµç‡15%+è¢«è§†ä¸ºé«˜è´¨é‡ï¼‰
        ratio_score = 0
        if bookmark_ratio > 0.05:  # æ”¶è—ç‡è¶…è¿‡5%
            ratio_score += (bookmark_ratio - 0.05) * 2000  # æ¯è¶…è¿‡1%åŠ 20åˆ†
        if like_ratio > 0.15:      # ç‚¹èµç‡è¶…è¿‡15%
            ratio_score += (like_ratio - 0.15) * 500       # æ¯è¶…è¿‡1%åŠ 5åˆ†
        # 1.3 é«˜äº’åŠ¨ç‡ä½œå“é¢å¤–åŠ æˆï¼ˆé’ˆå¯¹æ–°ä½œå“æˆ–å°ä¼—ä¼˜è´¨ä½œå“ï¼‰
        if view_count < 1000 and bookmark_ratio > 0.1:  # ä½æµè§ˆä½†é«˜æ”¶è—ç‡
            ratio_score *= 1.5
        # 1.4 ç»¼åˆåŸºç¡€è´¨é‡å¾—åˆ†
        quality_score = absolute_score + ratio_score
        # 2. æ–°é²œåº¦åŠ æˆ
        if create_date := item.get("createDate"):
            try:
                clean_date = create_date.split("T")[0]
                create_time = datetime.strptime(clean_date, "%Y-%m-%d").replace(tzinfo=timezone.utc)
                days_old = (current_time - create_time).days
                # æ–°é²œåº¦å› å­ï¼ˆæ›´å¹³æ»‘çš„è¡°å‡æ›²çº¿ï¼‰
                if days_old <= 3:      # 3å¤©å†…
                    freshness_factor = 2.0
                elif days_old <= 7:    # ä¸€å‘¨å†…
                    freshness_factor = 1.6
                elif days_old <= 14:   # ä¸¤å‘¨å†…
                    freshness_factor = 1.3
                elif days_old <= 30:   # ä¸€ä¸ªæœˆå†…
                    freshness_factor = 1.15
                elif days_old <= 60:   # ä¸¤ä¸ªæœˆå†…
                    freshness_factor = 1.05
                elif days_old <= 90:   # ä¸‰ä¸ªæœˆå†…
                    freshness_factor = 1.0
                else:
                    # 90å¤©ä»¥ä¸Šï¼Œæ¯å¤š30å¤©è¡°å‡0.05ï¼Œæœ€ä½0.5
                    decay_factor = max(0, (days_old - 90) / 30) * 0.05
                    freshness_factor = max(0.5, 1.0 - decay_factor)
                quality_score *= freshness_factor
            except Exception:
                pass  # è·³è¿‡æ—¥æœŸè§£æé”™è¯¯
        scored_items.append((quality_score, item))
    return scored_items

def _process_search_results(
    raw_results: list,
    is_explicit_r18_request: bool,
    current_time: datetime
) -> list:
    """å¤„ç†åŸå§‹æœç´¢ç»“æœï¼ˆè¿‡æ»¤/R-18éªŒè¯/è¯„åˆ†æ’åºï¼‰"""
    # è¿‡æ»¤æ— æ•ˆç»“æœ
    all_results = [
        item for item in raw_results
        if item and isinstance(item, dict)
        and item.get("id")
        and item.get("isAdContainer", 0) == 0
    ]
    # R-18å†…å®¹è¿‡æ»¤
    filtered_results = []
    for item in all_results:
        tag_names = _extract_tag_names(item)
        if is_explicit_r18_request or not _is_r18_content(tag_names):
            filtered_results.append(item)
    # è´¨é‡è¯„åˆ†æ’åº
    scored_items = _calculate_quality_scores(filtered_results, current_time)
    scored_items.sort(key=lambda x: x[0], reverse=True)
    return [item for _, item in scored_items[:100]]  # è¿”å›å‰100å€™é€‰

def _clean_old_cache(current_timestamp: float):
    """æ¸…ç†è¿‡æœŸç¼“å­˜ä½œå“"""
    for pid, timestamp in list(RECENT_IMAGES.items()):
        if current_timestamp - timestamp > EXCLUDE_DURATION:
            del RECENT_IMAGES[pid]

def _select_best_image(candidates: list, is_explicit_r18_request: bool) -> dict:
    """ä»å€™é€‰ä½œå“ä¸­é€‰æ‹©æœ€ä½³ä½œå“ï¼ˆè€ƒè™‘å†å²ä½¿ç”¨ï¼‰"""
    current_timestamp = time.time()
    # ä½¿ç”¨é”æ¥ä¿æŠ¤å…¨å±€ç¼“å­˜
    with RECENT_IMAGES_LOCK:
        # 1. ä¼˜å…ˆé€‰æ‹©é«˜è´¨é‡ä¸”æœªä½¿ç”¨è¿‡çš„ä½œå“
        unused_high_quality = [
            item for item in candidates[:30] 
            if str(item["id"]) not in RECENT_IMAGES
        ]
        if unused_high_quality:
            return random.choice(unused_high_quality)
        # 2. æ¬¡é€‰ï¼šæ‰€æœ‰æœªä½¿ç”¨è¿‡çš„ä½œå“
        unused_all = [
            item for item in candidates
            if str(item["id"]) not in RECENT_IMAGES
        ]
        if unused_all:
            return random.choice(unused_all)
        # 3. ä¿åº•ï¼šä½¿ç”¨æœ€ä¹…æœªç”¨çš„ä½œå“
        _clean_old_cache(current_timestamp)
        oldest_pid = min(RECENT_IMAGES.items(), key=lambda x: x[1])[0] if RECENT_IMAGES else None
        return next(
            (item for item in candidates if str(item["id"]) == oldest_pid),
            candidates[0]
        )

def _replace_image_domain(url: str) -> str:
    """å°†Pixivå›¾ç‰‡åŸŸåæ›¿æ¢ä¸ºä»£ç†åŸŸåï¼Œå¹¶ç¡®ä¿æ–‡ä»¶æ ¼å¼å…¼å®¹"""
    if not url.startswith("http"):
        url = "https:" + url
    proxy_base = PROXY_URL.rstrip('/') + '/'
    # ä¿®å¤URLä¸­çš„è½¬ä¹‰å­—ç¬¦
    url = url.replace("%2F", "/").replace("%3A", ":")
    if "i.pximg.net" in url:
        url = url.replace("https://i.pximg.net", proxy_base.rstrip('/'))
    elif "pixiv.cat" in url:
        url = url.replace("https://pixiv.cat", proxy_base.rstrip('/'))
    # ç¡®ä¿æ–‡ä»¶æ ¼å¼å…¼å®¹ï¼ˆé¿å…WebPç­‰ä¸æ”¯æŒçš„æ ¼å¼ï¼‰
    if url.endswith('.webp'):
        url = url[:-5] + '.jpg'  # è½¬ä¸º jpg
    elif url.endswith('.gif') and 'ugoira' not in url:  # éåŠ¨å›¾GIFè½¬ä¸ºJPG
        url = url[:-4] + '.jpg'
    # æ›¿æ¢URLä¸­çš„ç‰¹æ®Šå­—ç¬¦ï¼ˆé˜²æ­¢è·¯å¾„é—®é¢˜ï¼‰
    url = url.replace(' ', '%20').replace('&', '%26').replace('?', '%3F')
    return url

async def _validate_and_build_response(
    selected: dict,
    is_explicit_r18_request: bool,
    encoded_tag: list
) -> dict:
    """è·å–ä½œå“è¯¦æƒ…å¹¶éªŒè¯R-18å†…å®¹"""
    # è·å–ä½œå“è¯¦æƒ…
    illust_id = selected["id"]
    illust_url = f"https://www.pixiv.net/ajax/illust/{illust_id}"
    headers = _build_pixiv_headers(encoded_tag)
    headers.update({"Referer": f"https://www.pixiv.net/artworks/{illust_id}"})
    async with aiohttp.ClientSession() as session, \
        session.get(
            illust_url,
            headers=headers,
            proxy=PROXY if USE_PROXY else None,
            timeout=20
        ) as response:
            if response.status != HTTPStatus.OK:
                raise Exception(f"è·å–ä½œå“è¯¦æƒ…å¤±è´¥: {response.status}")
            data = await response.json()
            if data.get("error"):
                raise Exception(f"ä½œå“è¯¦æƒ…é”™è¯¯: {data.get('message', 'æœªçŸ¥é”™è¯¯')}")
            # äºŒæ¬¡R-18éªŒè¯
            work_tags = _extract_tag_names(data["body"])
            if not is_explicit_r18_request and (_is_r18_content(work_tags)):
                raise Exception("æ£€æµ‹åˆ°R-18å†…å®¹ä½†æœªæ˜ç¡®è¯·æ±‚")
            # æ„å»ºè¿”å›ç»“æœ
            body = data["body"]
            return {
                "image_url": _replace_image_domain(body["urls"]["original"]),
                "pid": str(illust_id),
                "title": body["title"],
                "author": body["userName"],
                "author_id": body["userId"],
                "work_url": f"https://www.pixiv.net/artworks/{illust_id}",
                "preview_url": _replace_image_domain(body["urls"]["regular"]),
                "original_url": body["urls"]["original"],
                "stats": {
                    "bookmarks": selected.get("bookmarkCount", 0),
                    "likes": selected.get("likeCount", 0),
                    "views": selected.get("viewCount", 0)
                },
                "strategy_used": selected.get("strategy_used", "unknown")
            }

def _cleanup_recent_images():
    """æ¸…ç†è¶…è¿‡24å°æ—¶çš„å›¾ç‰‡ID"""
    now = time.time()
    for image_id, timestamp in list(RECENT_IMAGES.items()):
        if now - timestamp > 24 * 3600:  # 24å°æ—¶
            del RECENT_IMAGES[image_id]

async def _find_optimal_size(img, orig_width, orig_height, target_size_range):
    """æ‰¾åˆ°æœ€ä½³å°ºå¯¸ï¼Œä½¿95%è´¨é‡çš„JPEGæ¥è¿‘ç›®æ ‡å¤§å°èŒƒå›´"""
    min_size, max_size = target_size_range
    current_img = img.copy()
    # 1. å…ˆæµ‹è¯•åŸå§‹å°ºå¯¸
    buffer = io.BytesIO()
    current_img.save(buffer, format="JPEG", quality=95, optimize=True, progressive=True)
    current_size = buffer.tell()
    # 2. å¦‚æœåŸå§‹å°ºå¯¸åœ¨ç›®æ ‡èŒƒå›´å†…ï¼Œç›´æ¥è¿”å›
    if min_size <= current_size <= max_size:
        logger.info(f"ğŸ¯ åŸå§‹å°ºå¯¸å®Œç¾åŒ¹é…ç›®æ ‡: {current_size/1024/1024:.2f}MB")
        return current_img
    # 3. å¦‚æœåŸå§‹å°ºå¯¸å¤ªå¤§ï¼Œç¼©å°
    if current_size > max_size:
        scale = 0.9  # ç¼©å°æ¯”ä¾‹
        while current_size > max_size and scale > 0.5:
            new_width = int(orig_width * scale)
            new_height = int(orig_height * scale)
            resized_img = img.resize((new_width, new_height), Image.LANCZOS)
            buffer = io.BytesIO()
            resized_img.save(buffer, format="JPEG", quality=95, optimize=True, progressive=True)
            current_size = buffer.tell()
            logger.debug(f"ğŸ” å°ºå¯¸æµ‹è¯•: {new_width}x{new_height} â†’ {current_size/1024/1024:.2f}MB")
            if min_size <= current_size <= max_size:
                logger.info(f"ğŸ¯ æ‰¾åˆ°å®Œç¾å°ºå¯¸: {new_width}x{new_height} ({current_size/1024/1024:.2f}MB)")
                return resized_img
            scale -= 0.05
        logger.info(f"ğŸ“ å°ºå¯¸ç¼©å°è‡³: {current_img.size[0]}x{current_img.size[1]} ({current_size/1024/1024:.2f}MB)")
        return current_img
    # 4. å¦‚æœåŸå§‹å°ºå¯¸å¤ªå°ï¼Œå°è¯•å¢å¤§ï¼ˆä»…å½“åŸå§‹å°ºå¯¸å°äºç›®æ ‡æ—¶ï¼‰
    if current_size < min_size and orig_width < 4096 and orig_height < 4096:
        scale = 1.1  # å¢å¤§æ¯”ä¾‹
        best_img = current_img.copy()
        best_size = current_size
        while current_size < max_size and scale <= 1.5:
            new_width = min(int(orig_width * scale), 4096)
            new_height = min(int(orig_height * scale), 4096)
            resized_img = img.resize((new_width, new_height), Image.LANCZOS)
            buffer = io.BytesIO()
            resized_img.save(buffer, format="JPEG", quality=95, optimize=True, progressive=True)
            current_size = buffer.tell()
            logger.debug(f"ğŸ” å°ºå¯¸æ”¾å¤§æµ‹è¯•: {new_width}x{new_height} â†’ {current_size/1024/1024:.2f}MB")
            if current_size <= max_size:
                best_img = resized_img
                best_size = current_size
            if min_size <= current_size <= max_size:
                logger.info(f"ğŸ¯ æ‰¾åˆ°å®Œç¾æ”¾å¤§å°ºå¯¸: {new_width}x{new_height} ({current_size/1024/1024:.2f}MB)")
                return resized_img
            scale += 0.1
        if best_size > current_size:  # å¦‚æœæœ‰æ”¹è¿›
            logger.info(f"ğŸ“ˆ å°ºå¯¸ä¼˜åŒ–è‡³: {best_img.size[0]}x{best_img.size[1]} ({best_size/1024/1024:.2f}MB)")
            return best_img
    return current_img

async def _fine_tune_quality(img, target_size_range):
    """åœ¨æœ€ä½³å°ºå¯¸åŸºç¡€ä¸Šå¾®è°ƒè´¨é‡ï¼Œç²¾ç¡®åŒ¹é…ç›®æ ‡å¤§å°"""
    min_size, max_size = target_size_range
    # 1. å…ˆæµ‹è¯•95%è´¨é‡
    buffer = io.BytesIO()
    img.save(buffer, format="JPEG", quality=95, optimize=True, progressive=True)
    current_size = buffer.tell()
    # 2. å¦‚æœå·²ç»æ¥è¿‘ç›®æ ‡ï¼Œç›´æ¥è¿”å›
    if min_size <= current_size <= max_size:
        return buffer, 95, current_size
    # 3. å¦‚æœå¤ªå¤§ï¼Œé™ä½è´¨é‡
    if current_size > max_size:
        low, high = 70, 95
        best_quality = 90
        best_buffer = None
        for _ in range(8):
            mid = (low + high) // 2
            buffer = io.BytesIO()
            img.save(buffer, format="JPEG", quality=mid, optimize=True, progressive=True)
            size = buffer.tell()
            logger.debug(f"ğŸ” è´¨é‡å¾®è°ƒ: {mid}% â†’ {size/1024/1024:.2f}MB")
            if size <= max_size:
                best_quality = mid
                best_buffer = buffer
                low = mid + 1
            else:
                high = mid - 1
        if best_buffer and best_buffer.tell() >= min_size:
            return best_buffer, best_quality, best_buffer.tell()
    # 4. å¦‚æœå¤ªå°ï¼Œå°è¯•æ·»åŠ å…ƒæ•°æ®å¢åŠ æ–‡ä»¶å¤§å°ï¼ˆæ— æŸï¼‰
    elif current_size < min_size:
        # æ·»åŠ EXIFå…ƒæ•°æ®ï¼ˆæ— æŸå¢åŠ æ–‡ä»¶å¤§å°ï¼‰
        exif_data = b" " * int(min_size - current_size)
        buffer = io.BytesIO()
        img.save(buffer, format="JPEG", quality=95, optimize=True, progressive=True, exif=exif_data)
        if buffer.tell() <= max_size:
            logger.info(f"ğŸ·ï¸ é€šè¿‡EXIFå…ƒæ•°æ®ä¼˜åŒ–æ–‡ä»¶å¤§å°: {current_size/1024/1024:.2f}MB â†’ {buffer.tell()/1024/1024:.2f}MB")
            return buffer, 95, buffer.tell()
    # 5. è¿”å›æœ€æ¥è¿‘çš„ç»“æœ
    return buffer, 95, current_size
