import urllib.parse
import logging
import aiohttp
import aiofiles
import io
import time
import random
import os
import ssl
import asyncio
from http import HTTPStatus
from pathlib import Path
from PIL import Image
from ..utils.pixiv_utils import (
    _is_r18_request,
    _is_r18_content,
    _extract_tag_names,
    _build_search_strategies,
    _execute_search_strategy,
    _select_best_image,
    _validate_and_build_response
)
from ..config.config import (
    PROXY,
    USE_PROXY, 
    MAX_ATTEMPTS,
    MAX_DOWNLOAD_CHUNK, 
    DOWNLOAD_TIMEOUT
    )
# åŸºç¡€é¡¹ç›®ç›®å½•
BASE_DIR = Path(__file__).parent.parent.parent.absolute()
DATA_DIR = BASE_DIR / "data"
TEMP_DIR = DATA_DIR / "pixiv_temp"  # ä¸“ç”¨ä¸´æ—¶ç›®å½•

# åˆ›å»ºç›®å½•
DATA_DIR.mkdir(parents=True, exist_ok=True)
TEMP_DIR.mkdir(parents=True, exist_ok=True)

# è¿‘æœŸå›¾ç‰‡ç¼“å­˜æ’é™¤æœºåˆ¶
RECENT_IMAGES = {}

# åˆ›å»ºæ—¥å¿—
logger = logging.getLogger()
logging.basicConfig(level = logging.INFO,format = '%(asctime)s - %(name)s - %(levelname)s - %(message)s')

async def search_pixiv_by_tag(tags: list, max_results=10) -> dict:
    """é€šè¿‡è§’è‰²æ ‡ç­¾æœç´¢Pixivå›¾ç‰‡ï¼ˆæ™ºèƒ½é€‚åº”æ–°è§’è‰²/å†·é—¨è§’è‰²ï¼‰"""
    # 1. é¢„å¤„ç†æ ‡ç­¾å’Œæœç´¢æ¨¡å¼
    search_tag = " ".join(tags)
    logger.info(f"æœç´¢æ ‡ç­¾ï¼š{search_tag}")
    encoded_tag = urllib.parse.quote(search_tag)
    is_explicit_r18_request = _is_r18_request(tags)
    # 2. ä¸‰é˜¶æ®µç­–ç•¥é…ç½®
    strategies = _build_search_strategies()
    # 3. ä¸‰é˜¶æ®µæœç´¢é‡è¯•
    for strategy in strategies:
        for attempt in range(5):  # æ¯ä¸ªç­–ç•¥æœ€å¤šå°è¯•5æ¬¡
            try:
                # 4. æ‰§è¡Œç­–ç•¥æœç´¢
                results = await _execute_search_strategy(
                    search_tag, encoded_tag, strategy
                )
                filtered_results = [r for r in results if not _is_r18_content(_extract_tag_names(r)) or is_explicit_r18_request]
                if not filtered_results and not is_explicit_r18_request:
                    # éR-18è¯·æ±‚ä½†å…¨æ˜¯R-18å†…å®¹ï¼Œè°ƒæ•´ç­–ç•¥å‚æ•°
                    logger.info(f"ç­–ç•¥[{strategy['name']}]å…¨æ˜¯R-18å†…å®¹ï¼Œè°ƒæ•´å‚æ•°é‡è¯•")
                    strategy["params"]["mode"] = "safe"  # æ·»åŠ å®‰å…¨æ¨¡å¼å‚æ•°
                    continue  # é‡è¯•å½“å‰ç­–ç•¥
                # 5. å¤„ç†ç»“æœå¹¶é€‰æ‹©ä½œå“
                selected = _select_best_image(results, is_explicit_r18_request)
                # 6. è·å–ä½œå“è¯¦æƒ…å¹¶éªŒè¯
                return await _validate_and_build_response(
                    selected, is_explicit_r18_request, encoded_tag
                )
            except Exception as e:
                logger.warning(f"ç­–ç•¥[{strategy['name']}]å°è¯•#{attempt+1}å¤±è´¥: {str(e)}")
                # å®Œæˆ5æ¬¡å°è¯•
                if (attempt >= 4 or
                        (strategy is strategies[-1] and attempt >= 1)):  # æœ€åä¸€ä¸ªç­–ç•¥æ—¶ï¼Œåœ¨ç¬¬äºŒæ¬¡å¤±è´¥åé€€å‡º
                    break
        else:  # å¦‚æœå¾ªç¯ä¸æ˜¯å› ä¸ºbreakç»“æŸï¼Œåˆ™ç»§ç»­ä¸‹ä¸€ä¸ªç­–ç•¥
            continue
        break  # å› ä¸ºbreakè€Œé€€å‡ºï¼Œä¸å†ç»§ç»­å°è¯•å…¶ä»–ç­–ç•¥
    raise Exception("æ‰€æœ‰æœç´¢ç­–ç•¥å‡å¤±è´¥æˆ–æœç´¢å‡å‘½ä¸­é™åˆ¶çº§å†…å®¹è¯·é‡è¯•")

async def get_remote_file_size(url: str) -> int:
    """è·å–è¿œç¨‹æ–‡ä»¶å¤§å°ï¼Œé¿å…ä¸‹è½½å¤§æ–‡ä»¶"""
    try:
        proxy = PROXY if USE_PROXY else None
        headers = {
            "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36",
            "Referer": "https://www.pixiv.net/"
        }
        async with aiohttp.ClientSession() as session:
            async with session.head(
                url, headers=headers, proxy=proxy, timeout=aiohttp.ClientTimeout(total=10)
            ) as response:
                if response.status in (200, 206):
                    content_range = response.headers.get('Content-Range', '')
                    if content_range:
                        # ä»Content-Rangeä¸­æå–æ–‡ä»¶å¤§å°ï¼šbytes 0-0/12345678
                        return int(content_range.split('/')[-1])
                    content_length = response.headers.get('Content-Length')
                    if content_length:
                        return int(content_length)
                    else:
                        # å°è¯•GETè¯·æ±‚å‰1KB
                        headers['Range'] = 'bytes=0-1023'
                        async with session.get(
                            url, headers=headers, proxy=proxy, timeout=aiohttp.ClientTimeout(total=10)
                        ) as response:
                            if response.status in (200, 206):
                                content_length = response.headers.get('Content-Length')
                                if content_length:
                                    # ä¼°ç®—å®Œæ•´æ–‡ä»¶å¤§å°ï¼ˆ1024å­—èŠ‚æ˜¯å¤´éƒ¨ï¼Œæ€»å¤§å°é€šå¸¸å¤§äºå¤´éƒ¨ï¼‰
                                    estimated_size = int(content_length)
                                    return estimated_size * 10  # ç²—ç•¥ä¼°è®¡
                return 0
    except Exception as e:
        logger.warning(f"è·å–æ–‡ä»¶å¤§å°å¤±è´¥: {str(e)}")
        return 0

async def compress_image(file_path: Path, max_size: int = 10 * 1024 * 1024) -> Path:
    """æ™ºèƒ½å‹ç¼©å›¾ç‰‡ï¼Œæœ€å¤§åŒ–åˆ©ç”¨10MBä¸Šé™ä¿æŒè´¨é‡"""
    try:
        original_size = file_path.stat().st_size
        if original_size <= max_size:
            return file_path
        logger.warning(f"âš ï¸ å›¾ç‰‡è¿‡å¤§ ({original_size/1024/1024:.2f}MB)ï¼Œå¼€å§‹æ™ºèƒ½å‹ç¼©...")
        with Image.open(file_path) as img:
            # 1. é¢„å¤„ç†ï¼šè½¬æ¢ä¸ºRGBï¼ˆç§»é™¤é€æ˜é€šé“ç­‰ï¼‰
            if img.mode in ('RGBA', 'LA', 'P'):
                background = Image.new('RGB', img.size, (255, 255, 255))
                background.paste(img, mask=img.split()[-1] if img.mode == 'RGBA' else None)
                img = background
            # 2. è·å–åŸå§‹å°ºå¯¸
            orig_width, orig_height = img.size
            max_dimension = 4096  # æœ€å¤§å…è®¸å°ºå¯¸
            # 3. åˆ¤æ–­æ˜¯å¦éœ€è¦è°ƒæ•´å°ºå¯¸
            need_resize = max(orig_width, orig_height) > max_dimension
            if need_resize:
                ratio = max_dimension / max(orig_width, orig_height)
                new_size = (int(orig_width * ratio), int(orig_height * ratio))
                img = img.resize(new_size, Image.LANCZOS)
                logger.info(f"ğŸ“ è°ƒæ•´å°ºå¯¸: {orig_width}x{orig_height} â†’ {new_size[0]}x{new_size[1]}")
            # 4. å°è¯•ä»…é€šè¿‡è°ƒæ•´JPEGè´¨é‡æ¥å‹ç¼©å›¾ç‰‡
            buffer = io.BytesIO()
            img.save(buffer, format="JPEG", quality=95, optimize=True, progressive=True)
            size_after_quality_adjustment = buffer.tell()
            if size_after_quality_adjustment <= max_size:
                logger.debug(f"ğŸ” å•çº¯è°ƒæ•´è´¨é‡å·²æ»¡è¶³è¦æ±‚: {size_after_quality_adjustment/1024/1024:.2f}MB")
                compressed_size = size_after_quality_adjustment
                best_buffer = buffer
                best_quality = 95
            else:
                # 5. ä½¿ç”¨äºŒåˆ†æŸ¥æ‰¾æ³•å¯»æ‰¾æœ€ä½³JPEGè´¨é‡
                low, high = 70, 98  # åˆç†è´¨é‡èŒƒå›´
                best_quality = 85  # é»˜è®¤è´¨é‡
                best_buffer = None
                target_size = max_size * 0.95
                for _ in range(8):  # æœ€å¤š8æ¬¡è¿­ä»£
                    mid = (low + high) // 2
                    buffer = io.BytesIO()
                    img.save(buffer, format="JPEG", quality=mid, optimize=True, progressive=True)
                    size = buffer.tell()
                    logger.debug(f"ğŸ” è´¨é‡æµ‹è¯•: {mid}% â†’ {size/1024/1024:.2f}MB")
                    if size <= target_size:
                        best_quality = mid
                        best_buffer = buffer
                        low = mid + 1  # å°è¯•æ›´é«˜å“è´¨
                    else:
                        high = mid - 1
                compressed_size = best_buffer.tell()
            # 6. éªŒè¯æœ€ç»ˆç»“æœå¹¶ä¿å­˜
            if best_buffer and compressed_size <= max_size:
                new_file_path = file_path.with_name(f"{file_path.stem}_compressed.jpg")
                with open(new_file_path, 'wb') as f:
                    f.write(best_buffer.getvalue())
                logger.info(
                    f"âœ… å‹ç¼©æˆåŠŸ: {original_size/1024/1024:.2f}MB â†’ "
                    f"{compressed_size/1024/1024:.2f}MB "
                    f"(è´¨é‡: {best_quality}%, å°ºå¯¸: {img.size[0]}x{img.size[1]})"
                )
                return new_file_path
            logger.warning("âš ï¸ æ™ºèƒ½å‹ç¼©æœªè¾¾ç›®æ ‡ï¼Œä½¿ç”¨é¢„è§ˆå›¾æ›¿ä»£")
            return None
    except Exception as e:
        logger.error(f"å›¾ç‰‡å‹ç¼©å¤±è´¥: {str(e)}", exc_info=True)
        return None

async def download_original_image(url: str) -> Path:
    """å®‰å…¨ä¸‹è½½å¤§æ–‡ä»¶åˆ°ä¸´æ—¶ä½ç½®ï¼Œè¿”å›æ–‡ä»¶è·¯å¾„ï¼ˆç¡®ä¿ä¸è¶…è¿‡10MBï¼‰"""
    file_size = await get_remote_file_size(url)
    # ç”Ÿæˆå”¯ä¸€æ–‡ä»¶å
    timestamp = int(time.time() * 1000)
    random_str = ''.join(random.choices('abcdefghijklmnopqrstuvwxyz0123456789', k=8))
    parsed_url = urllib.parse.urlparse(url)
    ext = os.path.splitext(parsed_url.path)[1] or '.jpg'
    # ç¡®ä¿æ–‡ä»¶æ‰©å±•åå…¼å®¹
    ext = ext.lower()
    if ext in ['.webp', '.avif', '.heic']:
        ext = '.jpg'
    elif ext == '.svg':
        ext = '.png'
    filename = f"pixiv_{timestamp}_{random_str}{ext}"
    temp_path = TEMP_DIR / filename
    logger.info(f"å¼€å§‹ä¸‹è½½åŸå›¾åˆ°: {temp_path} (é¢„ä¼°å¤§å°: {file_size/1024/1024:.2f}MB)")
    proxy = PROXY if USE_PROXY else None
    headers = {
        "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36",
        "Referer": "https://www.pixiv.net/"
    }
    # åˆ›å»ºSSLä¸Šä¸‹æ–‡ï¼ˆé¿å…SSLéªŒè¯é—®é¢˜ï¼‰
    ssl_context = ssl.create_default_context()
    ssl_context.check_hostname = False
    ssl_context.verify_mode = ssl.CERT_NONE
    # é‡è¯•æœºåˆ¶
    for attempt in range(MAX_ATTEMPTS):
        try:
            async with aiohttp.ClientSession() as session, \
                session.get(
                    url, headers=headers, proxy=proxy,
                    timeout=aiohttp.ClientTimeout(total=DOWNLOAD_TIMEOUT),
                    ssl=ssl_context
                ) as response:
                    if response.status != HTTPStatus.OK:
                        error_text = await response.text()
                        raise Exception(f"ä¸‹è½½å¤±è´¥ï¼ŒçŠ¶æ€ç : {response.status}, å“åº”: {error_text[:200]}")
                    # åˆ†å—å†™å…¥æ–‡ä»¶ï¼Œé¿å…å†…å­˜æº¢å‡º
                    total_bytes = 0
                    start_time = time.time()
                    async with aiofiles.open(temp_path, 'wb') as f:
                        async for chunk in response.content.iter_chunked(MAX_DOWNLOAD_CHUNK):
                            await f.write(chunk)
                            total_bytes += len(chunk)
                    # éªŒè¯æ–‡ä»¶å®Œæ•´æ€§
                    downloaded_size = temp_path.stat().st_size
                    if file_size > 0 and downloaded_size < file_size * 0.9:
                        raise Exception(f"æ–‡ä»¶ä¸å®Œæ•´: æœŸæœ› {file_size} å­—èŠ‚, å®é™… {downloaded_size} å­—èŠ‚")
                    # ä½¿ç”¨PillowéªŒè¯å›¾ç‰‡
                    try:
                        with Image.open(temp_path) as img:
                            img.verify()  # éªŒè¯æ˜¯å¦ä¸ºæœ‰æ•ˆçš„å›¾ç‰‡æ ¼å¼
                    except Exception as e:
                        logger.warning(f"å›¾ç‰‡éªŒè¯å¤±è´¥ï¼Œå°è¯•ä¿®å¤: {str(e)}")
                        # å°è¯•ä¿®å¤ï¼šé‡å‘½åæ‰©å±•å
                        if not str(temp_path).lower().endswith(('.jpg', '.jpeg', '.png')):
                            new_path = temp_path.with_suffix('.jpg')
                            temp_path.rename(new_path)
                            temp_path = new_path
                    # æ£€æŸ¥æ–‡ä»¶å¤§å°å¹¶å‹ç¼©ï¼ˆå¦‚æœéœ€è¦ï¼‰
                    if downloaded_size > 10 * 1024 * 1024:  # è¶…è¿‡10MB
                        logger.warning(f"âš ï¸ å›¾ç‰‡è¿‡å¤§ ({downloaded_size/1024/1024:.1f}MB)ï¼Œå°è¯•å‹ç¼©...")
                        compressed_path = await compress_image(temp_path)
                        if compressed_path:
                            temp_path = compressed_path
                            logger.info(f"âœ… å›¾ç‰‡å·²å‹ç¼©è‡³ {temp_path.stat().st_size/1024/1024:.2f}MB")
                        else:
                            logger.warning("âš ï¸ å›¾ç‰‡å‹ç¼©å¤±è´¥ï¼Œå°†ä½¿ç”¨é¢„è§ˆå›¾")
                            return None  # è¿”å›Noneè¡¨ç¤ºéœ€è¦ä½¿ç”¨é¢„è§ˆå›¾
                    logger.info(f"âœ… åŸå›¾ä¸‹è½½æˆåŠŸ: {downloaded_size/1024/1024:.2f}MB, è€—æ—¶: {time.time()-start_time:.1f}s")
                    return temp_path
        except Exception as e:
            logger.error(f"ä¸‹è½½å°è¯• {attempt+1}/{MAX_ATTEMPTS} å¤±è´¥: {str(e)}")
            if attempt == MAX_ATTEMPTS - 1:
                raise
            await asyncio.sleep(2)
    # å¦‚æœæ²¡æœ‰è¿”å›ï¼Œè¿”å›ä¸´æ—¶è·¯å¾„
    return temp_path

async def cleanup_temp_files():
    """æ¸…ç†6å°æ—¶ä»¥ä¸Šçš„ä¸´æ—¶æ–‡ä»¶"""
    try:
        now = time.time()
        for file_path in TEMP_DIR.glob("*"):
            if file_path.is_file():
                file_age = now - file_path.stat().st_mtime
                if file_age > 6 * 3600:  # 6å°æ—¶
                    try:
                        file_path.unlink()
                        logger.debug(f"æ¸…ç†æ—§ä¸´æ—¶æ–‡ä»¶: {file_path.name}")
                    except Exception as e:
                        logger.warning(f"æ¸…ç†æ–‡ä»¶å¤±è´¥ {file_path.name}: {str(e)}")
    except Exception as e:
        logger.warning(f"æ¸…ç†ä¸´æ—¶æ–‡ä»¶æ—¶å‡ºé”™: {str(e)}")

async def download_and_process_preview(image_url: str) -> bytes:
    """ä¸‹è½½å¹¶å¤„ç†é¢„è§ˆå›¾ï¼ˆå°å°ºå¯¸ï¼‰"""
    try:
        proxy = PROXY if USE_PROXY else None
        headers = {
            "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36",
            "Referer": "https://www.pixiv.net/"
        }
        async with aiohttp.ClientSession() as session,\
            session.get(
                image_url, headers=headers, proxy=proxy, timeout=aiohttp.ClientTimeout(total=15)
            ) as response:
                if response.status != 200:
                    raise Exception(f"é¢„è§ˆå›¾ä¸‹è½½å¤±è´¥ï¼ŒçŠ¶æ€ç : {response.status}")
                return await response.read()
    except Exception as e:
        logger.error(f"é¢„è§ˆå›¾å¤„ç†å¤±è´¥: {str(e)}")
        raise Exception(f"é¢„è§ˆå›¾å¤„ç†å¤±è´¥: {str(e)}")