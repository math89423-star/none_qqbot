import urllib.parse
import logging
import aiohttp
import aiofiles
import io
import time
import random
import os
import ssl
import asyncio
from http import HTTPStatus
from pathlib import Path
from PIL import Image
from ..utils.pixiv_utils import (
    _is_r18_request,
    _is_r18_content,
    _extract_tag_names,
    _build_search_strategies,
    _execute_search_strategy,
    _select_best_image,
    _validate_and_build_response
)
from ..config.config import (
    PROXY,
    USE_PROXY, 
    MAX_ATTEMPTS,
    MAX_DOWNLOAD_CHUNK, 
    DOWNLOAD_TIMEOUT
    )
# åŸºç¡€é¡¹ç›®ç›®å½•
BASE_DIR = Path(__file__).parent.parent.parent.absolute()
DATA_DIR = BASE_DIR / "data"
TEMP_DIR = DATA_DIR / "pixiv_temp"  # ä¸“ç”¨ä¸´æ—¶ç›®å½•

# åˆ›å»ºç›®å½•
DATA_DIR.mkdir(parents=True, exist_ok=True)
TEMP_DIR.mkdir(parents=True, exist_ok=True)

# è¿‘æœŸå›¾ç‰‡ç¼“å­˜æ’é™¤æœºåˆ¶
RECENT_IMAGES = {}

# åˆ›å»ºæ—¥å¿—
logger = logging.getLogger()
logging.basicConfig(level = logging.INFO,format = '%(asctime)s - %(name)s - %(levelname)s - %(message)s')

async def search_pixiv_by_tag(tags: list, max_results=10) -> dict:
    """é€šè¿‡è§’è‰²æ ‡ç­¾æœç´¢Pixivå›¾ç‰‡ï¼ˆæ™ºèƒ½é€‚åº”æ–°è§’è‰²/å†·é—¨è§’è‰²ï¼‰"""
    # 1. é¢„å¤„ç†æ ‡ç­¾å’Œæœç´¢æ¨¡å¼
    search_tag = " ".join(tags)
    logger.info(f"æœç´¢æ ‡ç­¾ï¼š{search_tag}")
    encoded_tag = urllib.parse.quote(search_tag)
    is_explicit_r18_request = _is_r18_request(tags)
    # 2. ä¸‰é˜¶æ®µç­–ç•¥é…ç½®
    strategies = _build_search_strategies()
    # 3. ä¸‰é˜¶æ®µæœç´¢é‡è¯•
    for strategy in strategies:
        for attempt in range(5):  # æ¯ä¸ªç­–ç•¥æœ€å¤šå°è¯•5æ¬¡
            try:
                # 4. æ‰§è¡Œç­–ç•¥æœç´¢
                results = await _execute_search_strategy(
                    search_tag, encoded_tag, strategy
                )
                filtered_results = [r for r in results if not _is_r18_content(_extract_tag_names(r)) or is_explicit_r18_request]
                if not filtered_results and not is_explicit_r18_request:
                    # éR-18è¯·æ±‚ä½†å…¨æ˜¯R-18å†…å®¹ï¼Œè°ƒæ•´ç­–ç•¥å‚æ•°
                    logger.info(f"ç­–ç•¥[{strategy['name']}]å…¨æ˜¯R-18å†…å®¹ï¼Œè°ƒæ•´å‚æ•°é‡è¯•")
                    strategy["params"]["mode"] = "safe"  # æ·»åŠ å®‰å…¨æ¨¡å¼å‚æ•°
                    continue  # é‡è¯•å½“å‰ç­–ç•¥
                # 5. å¤„ç†ç»“æœå¹¶é€‰æ‹©ä½œå“
                selected = _select_best_image(results, is_explicit_r18_request)
                # 6. è·å–ä½œå“è¯¦æƒ…å¹¶éªŒè¯
                return await _validate_and_build_response(
                    selected, is_explicit_r18_request, encoded_tag
                )
            except Exception as e:
                logger.warning(f"ç­–ç•¥[{strategy['name']}]å°è¯•#{attempt+1}å¤±è´¥: {str(e)}")
                # å¦‚æœæ˜¯æœ€åä¸€æ¬¡å°è¯•æˆ–æœ€åä¸€ä¸ªç­–ç•¥ä¸”ç¬¬äºŒæ¬¡å°è¯•åä»å¤±è´¥ï¼Œåˆ™é€€å‡ºå½“å‰ç­–ç•¥å¾ªç¯
                if attempt == 4 or (strategy is strategies[-1] and attempt >= 1):
                    break
        # å¦‚æœæ˜¯å› ä¸ºå†…éƒ¨å¾ªç¯æ­£å¸¸ç»“æŸï¼ˆæœªè§¦å‘breakï¼‰ï¼Œåˆ™ç»§ç»­ä¸‹ä¸€ä¸ªç­–ç•¥
        else:
            continue
        # å› ä¸ºbreakè€Œé€€å‡ºï¼Œä¸å†ç»§ç»­å°è¯•å…¶ä»–ç­–ç•¥
        break
    raise Exception("æ‰€æœ‰æœç´¢ç­–ç•¥å‡å¤±è´¥æˆ–æœç´¢å‡å‘½ä¸­é™åˆ¶çº§å†…å®¹è¯·é‡è¯•")

async def get_remote_file_size(url: str) -> int:
    """è·å–è¿œç¨‹æ–‡ä»¶å¤§å°ï¼Œé¿å…ä¸‹è½½å¤§æ–‡ä»¶"""
    try:
        proxy = PROXY if USE_PROXY else None
        headers = {
            "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36",
            "Referer": "https://www.pixiv.net/"
        }
        async with aiohttp.ClientSession() as session:
            async with session.head(
                url, headers=headers, proxy=proxy, timeout=aiohttp.ClientTimeout(total=10)
            ) as response:
                if response.status in (200, 206):
                    content_range = response.headers.get('Content-Range', '')
                    if content_range:
                        # ä»Content-Rangeä¸­æå–æ–‡ä»¶å¤§å°ï¼šbytes 0-0/12345678
                        return int(content_range.split('/')[-1])
                    content_length = response.headers.get('Content-Length')
                    if content_length:
                        return int(content_length)
                    else:
                        # å°è¯•GETè¯·æ±‚å‰1KB
                        headers['Range'] = 'bytes=0-1023'
                        async with session.get(
                            url, headers=headers, proxy=proxy, timeout=aiohttp.ClientTimeout(total=10)
                        ) as response:
                            if response.status in (200, 206):
                                content_length = response.headers.get('Content-Length')
                                if content_length:
                                    # ä¼°ç®—å®Œæ•´æ–‡ä»¶å¤§å°ï¼ˆ1024å­—èŠ‚æ˜¯å¤´éƒ¨ï¼Œæ€»å¤§å°é€šå¸¸å¤§äºå¤´éƒ¨ï¼‰
                                    estimated_size = int(content_length)
                                    return estimated_size * 10  # ç²—ç•¥ä¼°è®¡
                return 0
    except Exception as e:
        logger.warning(f"è·å–æ–‡ä»¶å¤§å°å¤±è´¥: {str(e)}")
        return 0

async def compress_image(file_path: Path, max_size: int = 10 * 1024 * 1024) -> Path:
    """æ™ºèƒ½å‹ç¼©å›¾ç‰‡ï¼Œæœ€å¤§åŒ–åˆ©ç”¨10MBä¸Šé™ä¿æŒè´¨é‡"""
    try:
        original_size = file_path.stat().st_size
        if original_size <= max_size:
            return file_path
        logger.warning(f"âš ï¸ å›¾ç‰‡è¿‡å¤§ ({original_size/1024/1024:.2f}MB)ï¼Œå¼€å§‹æ™ºèƒ½å‹ç¼©...")
        with Image.open(file_path) as img:
            # 1. é¢„å¤„ç†ï¼šè½¬æ¢ä¸ºRGB
            if img.mode in ('RGBA', 'LA', 'P'):
                background = Image.new('RGB', img.size, (255, 255, 255))
                background.paste(img, mask=img.split()[-1] if img.mode == 'RGBA' else None)
                img = background
            orig_width, orig_height = img.size
            target_size_range = (max_size * 0.9, max_size * 0.98)  # ç›®æ ‡èŒƒå›´: 9-9.8MB
            # 2. é˜¶æ®µ1: å°ºå¯¸ä¼˜åŒ– - æ‰¾åˆ°æœ€ä½³å°ºå¯¸
            optimal_img = await _find_optimal_size(img, orig_width, orig_height, target_size_range)
            # 3. é˜¶æ®µ2: è´¨é‡å¾®è°ƒ - åœ¨æœ€ä½³å°ºå¯¸åŸºç¡€ä¸Šè°ƒæ•´è´¨é‡
            result = await _fine_tune_quality(optimal_img, target_size_range)
            # 4. ä¿å­˜æœ€ç»ˆç»“æœ
            if result:
                compressed_img, best_quality, compressed_size = result
                new_file_path = file_path.with_name(f"{file_path.stem}_compressed.jpg")
                with open(new_file_path, 'wb') as f:
                    f.write(compressed_img.getvalue())
                logger.info(
                    f"âœ… å‹ç¼©æˆåŠŸ: {original_size/1024/1024:.2f}MB â†’ "
                    f"{compressed_size/1024/1024:.2f}MB "
                    f"(è´¨é‡: {best_quality}%, å°ºå¯¸: {optimal_img.size[0]}x{optimal_img.size[1]})"
                )
                return new_file_path
            logger.warning("âš ï¸ æ™ºèƒ½å‹ç¼©æœªè¾¾ç›®æ ‡ï¼Œä½¿ç”¨é¢„è§ˆå›¾æ›¿ä»£")
            return None
    except Exception as e:
        logger.error(f"å›¾ç‰‡å‹ç¼©å¤±è´¥: {str(e)}", exc_info=True)
        return None

async def _find_optimal_size(img, orig_width, orig_height, target_size_range):
    """æ‰¾åˆ°æœ€ä½³å°ºå¯¸ï¼Œä½¿95%è´¨é‡çš„JPEGæ¥è¿‘ç›®æ ‡å¤§å°èŒƒå›´"""
    min_size, max_size = target_size_range
    current_img = img.copy()
    # 1. å…ˆæµ‹è¯•åŸå§‹å°ºå¯¸
    buffer = io.BytesIO()
    current_img.save(buffer, format="JPEG", quality=95, optimize=True, progressive=True)
    current_size = buffer.tell()
    # 2. å¦‚æœåŸå§‹å°ºå¯¸åœ¨ç›®æ ‡èŒƒå›´å†…ï¼Œç›´æ¥è¿”å›
    if min_size <= current_size <= max_size:
        logger.info(f"ğŸ¯ åŸå§‹å°ºå¯¸å®Œç¾åŒ¹é…ç›®æ ‡: {current_size/1024/1024:.2f}MB")
        return current_img
    # 3. å¦‚æœåŸå§‹å°ºå¯¸å¤ªå¤§ï¼Œç¼©å°
    if current_size > max_size:
        scale = 0.9  # ç¼©å°æ¯”ä¾‹
        while current_size > max_size and scale > 0.5:
            new_width = int(orig_width * scale)
            new_height = int(orig_height * scale)
            resized_img = img.resize((new_width, new_height), Image.LANCZOS)
            buffer = io.BytesIO()
            resized_img.save(buffer, format="JPEG", quality=95, optimize=True, progressive=True)
            current_size = buffer.tell()
            logger.debug(f"ğŸ” å°ºå¯¸æµ‹è¯•: {new_width}x{new_height} â†’ {current_size/1024/1024:.2f}MB")
            if min_size <= current_size <= max_size:
                logger.info(f"ğŸ¯ æ‰¾åˆ°å®Œç¾å°ºå¯¸: {new_width}x{new_height} ({current_size/1024/1024:.2f}MB)")
                return resized_img
            scale -= 0.05
        logger.info(f"ğŸ“ å°ºå¯¸ç¼©å°è‡³: {current_img.size[0]}x{current_img.size[1]} ({current_size/1024/1024:.2f}MB)")
        return current_img
    # 4. å¦‚æœåŸå§‹å°ºå¯¸å¤ªå°ï¼Œå°è¯•å¢å¤§ï¼ˆä»…å½“åŸå§‹å°ºå¯¸å°äºç›®æ ‡æ—¶ï¼‰
    if current_size < min_size and orig_width < 4096 and orig_height < 4096:
        scale = 1.1  # å¢å¤§æ¯”ä¾‹
        best_img = current_img.copy()
        best_size = current_size
        while current_size < max_size and scale <= 1.5:
            new_width = min(int(orig_width * scale), 4096)
            new_height = min(int(orig_height * scale), 4096)
            resized_img = img.resize((new_width, new_height), Image.LANCZOS)
            buffer = io.BytesIO()
            resized_img.save(buffer, format="JPEG", quality=95, optimize=True, progressive=True)
            current_size = buffer.tell()
            logger.debug(f"ğŸ” å°ºå¯¸æ”¾å¤§æµ‹è¯•: {new_width}x{new_height} â†’ {current_size/1024/1024:.2f}MB")
            if current_size <= max_size:
                best_img = resized_img
                best_size = current_size
            if min_size <= current_size <= max_size:
                logger.info(f"ğŸ¯ æ‰¾åˆ°å®Œç¾æ”¾å¤§å°ºå¯¸: {new_width}x{new_height} ({current_size/1024/1024:.2f}MB)")
                return resized_img
            scale += 0.1
        if best_size > current_size:  # å¦‚æœæœ‰æ”¹è¿›
            logger.info(f"ğŸ“ˆ å°ºå¯¸ä¼˜åŒ–è‡³: {best_img.size[0]}x{best_img.size[1]} ({best_size/1024/1024:.2f}MB)")
            return best_img
    return current_img

async def _fine_tune_quality(img, target_size_range):
    """åœ¨æœ€ä½³å°ºå¯¸åŸºç¡€ä¸Šå¾®è°ƒè´¨é‡ï¼Œç²¾ç¡®åŒ¹é…ç›®æ ‡å¤§å°"""
    min_size, max_size = target_size_range
    # 1. å…ˆæµ‹è¯•95%è´¨é‡
    buffer = io.BytesIO()
    img.save(buffer, format="JPEG", quality=95, optimize=True, progressive=True)
    current_size = buffer.tell()
    # 2. å¦‚æœå·²ç»æ¥è¿‘ç›®æ ‡ï¼Œç›´æ¥è¿”å›
    if min_size <= current_size <= max_size:
        return buffer, 95, current_size
    # 3. å¦‚æœå¤ªå¤§ï¼Œé™ä½è´¨é‡
    if current_size > max_size:
        low, high = 70, 95
        best_quality = 90
        best_buffer = None
        for _ in range(8):
            mid = (low + high) // 2
            buffer = io.BytesIO()
            img.save(buffer, format="JPEG", quality=mid, optimize=True, progressive=True)
            size = buffer.tell()
            logger.debug(f"ğŸ” è´¨é‡å¾®è°ƒ: {mid}% â†’ {size/1024/1024:.2f}MB")
            if size <= max_size:
                best_quality = mid
                best_buffer = buffer
                low = mid + 1
            else:
                high = mid - 1
        if best_buffer and best_buffer.tell() >= min_size:
            return best_buffer, best_quality, best_buffer.tell()
    # 4. å¦‚æœå¤ªå°ï¼Œå°è¯•æ·»åŠ å…ƒæ•°æ®å¢åŠ æ–‡ä»¶å¤§å°ï¼ˆæ— æŸï¼‰
    elif current_size < min_size:
        # æ·»åŠ EXIFå…ƒæ•°æ®ï¼ˆæ— æŸå¢åŠ æ–‡ä»¶å¤§å°ï¼‰
        exif_data = b" " * int(min_size - current_size)
        buffer = io.BytesIO()
        img.save(buffer, format="JPEG", quality=95, optimize=True, progressive=True, exif=exif_data)
        if buffer.tell() <= max_size:
            logger.info(f"ğŸ·ï¸ é€šè¿‡EXIFå…ƒæ•°æ®ä¼˜åŒ–æ–‡ä»¶å¤§å°: {current_size/1024/1024:.2f}MB â†’ {buffer.tell()/1024/1024:.2f}MB")
            return buffer, 95, buffer.tell()
    # 5. è¿”å›æœ€æ¥è¿‘çš„ç»“æœ
    return buffer, 95, current_size

async def download_original_image(url: str) -> Path:
    """å®‰å…¨ä¸‹è½½å¤§æ–‡ä»¶åˆ°ä¸´æ—¶ä½ç½®ï¼Œè¿”å›æ–‡ä»¶è·¯å¾„ï¼ˆç¡®ä¿ä¸è¶…è¿‡10MBï¼‰"""
    file_size = await get_remote_file_size(url)
    # ç”Ÿæˆå”¯ä¸€æ–‡ä»¶å
    timestamp = int(time.time() * 1000)
    random_str = ''.join(random.choices('abcdefghijklmnopqrstuvwxyz0123456789', k=8))
    parsed_url = urllib.parse.urlparse(url)
    ext = os.path.splitext(parsed_url.path)[1] or '.jpg'
    # ç¡®ä¿æ–‡ä»¶æ‰©å±•åå…¼å®¹
    ext = ext.lower()
    if ext in ['.webp', '.avif', '.heic']:
        ext = '.jpg'
    elif ext == '.svg':
        ext = '.png'
    filename = f"pixiv_{timestamp}_{random_str}{ext}"
    temp_path = TEMP_DIR / filename
    logger.info(f"å¼€å§‹ä¸‹è½½åŸå›¾åˆ°: {temp_path} (é¢„ä¼°å¤§å°: {file_size/1024/1024:.2f}MB)")
    proxy = PROXY if USE_PROXY else None
    headers = {
        "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36",
        "Referer": "https://www.pixiv.net/"
    }
    # åˆ›å»ºSSLä¸Šä¸‹æ–‡ï¼ˆé¿å…SSLéªŒè¯é—®é¢˜ï¼‰
    ssl_context = ssl.create_default_context()
    ssl_context.check_hostname = False
    ssl_context.verify_mode = ssl.CERT_NONE
    # é‡è¯•æœºåˆ¶
    for attempt in range(MAX_ATTEMPTS):
        try:
            async with aiohttp.ClientSession() as session, \
                session.get(
                    url, headers=headers, proxy=proxy,
                    timeout=aiohttp.ClientTimeout(total=DOWNLOAD_TIMEOUT),
                    ssl=ssl_context
                ) as response:
                    if response.status != HTTPStatus.OK:
                        error_text = await response.text()
                        raise Exception(f"ä¸‹è½½å¤±è´¥ï¼ŒçŠ¶æ€ç : {response.status}, å“åº”: {error_text[:200]}")
                    # åˆ†å—å†™å…¥æ–‡ä»¶ï¼Œé¿å…å†…å­˜æº¢å‡º
                    total_bytes = 0
                    start_time = time.time()
                    async with aiofiles.open(temp_path, 'wb') as f:
                        async for chunk in response.content.iter_chunked(MAX_DOWNLOAD_CHUNK):
                            await f.write(chunk)
                            total_bytes += len(chunk)
                    # éªŒè¯æ–‡ä»¶å®Œæ•´æ€§
                    downloaded_size = temp_path.stat().st_size
                    if file_size > 0 and downloaded_size < file_size * 0.9:
                        raise Exception(f"æ–‡ä»¶ä¸å®Œæ•´: æœŸæœ› {file_size} å­—èŠ‚, å®é™… {downloaded_size} å­—èŠ‚")
                    # ä½¿ç”¨PillowéªŒè¯å›¾ç‰‡
                    try:
                        with Image.open(temp_path) as img:
                            img.verify()  # éªŒè¯æ˜¯å¦ä¸ºæœ‰æ•ˆçš„å›¾ç‰‡æ ¼å¼
                    except Exception as e:
                        logger.warning(f"å›¾ç‰‡éªŒè¯å¤±è´¥ï¼Œå°è¯•ä¿®å¤: {str(e)}")
                        # å°è¯•ä¿®å¤ï¼šé‡å‘½åæ‰©å±•å
                        if not str(temp_path).lower().endswith(('.jpg', '.jpeg', '.png')):
                            new_path = temp_path.with_suffix('.jpg')
                            temp_path.rename(new_path)
                            temp_path = new_path
                    # æ£€æŸ¥æ–‡ä»¶å¤§å°å¹¶å‹ç¼©ï¼ˆå¦‚æœéœ€è¦ï¼‰
                    if downloaded_size > 10 * 1024 * 1024:  # è¶…è¿‡10MB
                        logger.warning(f"âš ï¸ å›¾ç‰‡è¿‡å¤§ ({downloaded_size/1024/1024:.1f}MB)ï¼Œå°è¯•å‹ç¼©...")
                        compressed_path = await compress_image(temp_path)
                        if compressed_path:
                            temp_path = compressed_path
                            logger.info(f"âœ… å›¾ç‰‡å·²å‹ç¼©è‡³ {temp_path.stat().st_size/1024/1024:.2f}MB")
                        else:
                            logger.warning("âš ï¸ å›¾ç‰‡å‹ç¼©å¤±è´¥ï¼Œå°†ä½¿ç”¨é¢„è§ˆå›¾")
                            return None  # è¿”å›Noneè¡¨ç¤ºéœ€è¦ä½¿ç”¨é¢„è§ˆå›¾
                    logger.info(f"âœ… åŸå›¾ä¸‹è½½æˆåŠŸ: {downloaded_size/1024/1024:.2f}MB, è€—æ—¶: {time.time()-start_time:.1f}s")
                    return temp_path
        except Exception as e:
            logger.error(f"ä¸‹è½½å°è¯• {attempt+1}/{MAX_ATTEMPTS} å¤±è´¥: {str(e)}")
            if attempt == MAX_ATTEMPTS - 1:
                raise
            await asyncio.sleep(2)
    # å¦‚æœæ²¡æœ‰è¿”å›ï¼Œè¿”å›ä¸´æ—¶è·¯å¾„
    return temp_path

async def cleanup_temp_files():
    """æ¸…ç†6å°æ—¶ä»¥ä¸Šçš„ä¸´æ—¶æ–‡ä»¶"""
    try:
        now = time.time()
        for file_path in TEMP_DIR.glob("*"):
            if file_path.is_file():
                file_age = now - file_path.stat().st_mtime
                if file_age > 6 * 3600:  # 6å°æ—¶
                    try:
                        file_path.unlink()
                        logger.debug(f"æ¸…ç†æ—§ä¸´æ—¶æ–‡ä»¶: {file_path.name}")
                    except Exception as e:
                        logger.warning(f"æ¸…ç†æ–‡ä»¶å¤±è´¥ {file_path.name}: {str(e)}")
    except Exception as e:
        logger.warning(f"æ¸…ç†ä¸´æ—¶æ–‡ä»¶æ—¶å‡ºé”™: {str(e)}")

async def download_and_process_preview(image_url: str) -> bytes:
    """ä¸‹è½½å¹¶å¤„ç†é¢„è§ˆå›¾ï¼ˆå°å°ºå¯¸ï¼‰"""
    try:
        proxy = PROXY if USE_PROXY else None
        headers = {
            "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36",
            "Referer": "https://www.pixiv.net/"
        }
        async with aiohttp.ClientSession() as session,\
            session.get(
                image_url, headers=headers, proxy=proxy, timeout=aiohttp.ClientTimeout(total=15)
            ) as response:
                if response.status != 200:
                    raise Exception(f"é¢„è§ˆå›¾ä¸‹è½½å¤±è´¥ï¼ŒçŠ¶æ€ç : {response.status}")
                return await response.read()
    except Exception as e:
        logger.error(f"é¢„è§ˆå›¾å¤„ç†å¤±è´¥: {str(e)}")
        raise Exception(f"é¢„è§ˆå›¾å¤„ç†å¤±è´¥: {str(e)}")