import aiohttp
import json
import urllib.parse
import random
import time
import os
import aiofiles
from nonebot import on_command, logger, get_driver
from nonebot.adapters.onebot.v11 import MessageSegment, Bot, Event
import asyncio
import ssl
import traceback
from pathlib import Path
from datetime import datetime, timezone, timedelta

# ====== é‡è¦é…ç½®ï¼ˆå¿…é¡»ä¿®æ”¹ï¼‰ ======#
# ä»ç¯å¢ƒå˜é‡è·å–é…ç½®
env = get_driver().config
# ä¼˜å…ˆä½¿ç”¨ç¯å¢ƒå˜é‡ï¼Œå…¶æ¬¡ä½¿ç”¨é»˜è®¤å€¼
PROXY = getattr(env, "PROXY_ADDRESS", "http://127.0.0.1:7890")  # æœ¬åœ°ä»£ç†åœ°å€
USE_PROXY = getattr(env, "USE_PROXY", True)  # æ˜¯å¦ä½¿ç”¨ä»£ç†
PROXY_URL = getattr(env, "CF_WORKER_URL", "https://quiet-hill-31f3.math89423.workers.dev/")  # Cloudflare Workersåœ°å€
PIXIV_COOKIE = getattr(env, "PIXIV_COOKIE", "PHPSESSID=14916444_EuNtNE3Yd2ZZ50A7UzivUlxP7O2hLP7s; device_token=ccd49454e972c3b547f1db56a3560575; p_ab_id=1; p_ab_id_2=1")

# ===== æ–°å¢ï¼šå†·å´æœºåˆ¶é…ç½® =====
COOLDOWN_TIME = 25  # 25ç§’å†·å´æ—¶é—´
last_request_time = {}  # {user_id: last_request_time}

# åŸºç¡€é¡¹ç›®ç›®å½•
BASE_DIR = Path(__file__).parent.parent.parent.absolute()
DATA_DIR = BASE_DIR / "data"
TEMP_DIR = DATA_DIR / "pixiv_temp"  # ä¸“ç”¨ä¸´æ—¶ç›®å½•

# åˆ›å»ºç›®å½•
DATA_DIR.mkdir(parents=True, exist_ok=True)
TEMP_DIR.mkdir(parents=True, exist_ok=True)

# åŸå›¾å‘é€ä¸“ç”¨é…ç½®
MAX_DOWNLOAD_CHUNK = 8192  # 8KBåˆ†å—ä¸‹è½½
DOWNLOAD_TIMEOUT = 60  # 60ç§’è¶…æ—¶
MAX_ATTEMPTS = 2  # é‡è¯•æ¬¡æ•°

# ====== æ–°å¢ï¼šè¿‘æœŸå›¾ç‰‡ç¼“å­˜æ’é™¤æœºåˆ¶ ======
RECENT_IMAGES = {}  # {pid: last_used_time}
EXCLUDE_DURATION = 3600  # 1å°æ—¶å†…ä¸é‡å¤ä½¿ç”¨åŒä¸€ä½œå“

# ====== æ ¸å¿ƒå‡½æ•° ======
async def search_pixiv_by_tag(tags: list, max_results=10) -> dict:
    """é€šè¿‡è§’è‰²æ ‡ç­¾æœç´¢Pixivå›¾ç‰‡ï¼ˆä¼˜åŒ–é‡å¤ç‡ï¼Œæ·»åŠ R-18è¿‡æ»¤ï¼ŒæŒ‰çƒ­åº¦æ’åºä¸”é™åˆ¶è¿‘ä¸€å‘¨ï¼‰"""
    search_tag = " ".join(tags)
    encoded_tag = urllib.parse.quote(search_tag)
    
    # ===== å…³é”®ä¿®æ”¹1ï¼šæ£€æŸ¥æ˜¯å¦æ˜ç¡®è¯·æ±‚R-18å†…å®¹ =====
    is_explicit_r18_request = any(tag.lower() in ["r-18", "r18", "r-18g", "r18g"] for tag in tags)
    
    # ===== å…³é”®ä¿®æ”¹2ï¼šè®¾ç½®å®‰å…¨æ¨¡å¼å‚æ•° =====
    search_mode = "all" if is_explicit_r18_request else "safe"
    
    # ===== å…³é”®ä¼˜åŒ–ï¼šæŒ‰çƒ­åº¦æ’åº + è¿‘180å¤©æ—¶é—´èŒƒå›´ =====
    # è®¡ç®—è¿‘ä¸€å‘¨çš„æ—¥æœŸèŒƒå›´ (æ ¼å¼: YYYY-MM-DD)
    end_date = datetime.now().strftime("%Y-%m-%d")
    start_date = (datetime.now() - timedelta(days=180)).strftime("%Y-%m-%d")
    
    # ===== å…³é”®ä¿®æ”¹3ï¼šæ‰©å¤§éšæœºåç§»é‡èŒƒå›´ =====
    # è¦†ç›–3é¡µ(180å¼ )ä½œå“ï¼Œæ˜¾è‘—å¢åŠ å¤šæ ·æ€§
    offset = random.randint(0, 180)
    page = max(1, offset // 60 + 1)  # ç¡®ä¿pageè‡³å°‘ä¸º1
    
    url = f"https://www.pixiv.net/ajax/search/artworks/{encoded_tag}"
    params = {
        "word": search_tag,
        "order": "popular_d",  # æŒ‰çƒ­åº¦é™åºæ’åˆ—
        "mode": search_mode,  # ä½¿ç”¨å®‰å…¨æ¨¡å¼å‚æ•°
        "p": page,
        "s_mode": "s_tag",
        "type": "all",
        "lang": "zh",
        "scd": start_date,  # å¼€å§‹æ—¥æœŸ
        "ecd": end_date,  # ç»“æŸæ—¥æœŸ (ä»Šå¤©)
        "blt": "200"  # æœ€ä½æ”¶è—æ•° (è¿‡æ»¤ä½è´¨é‡ä½œå“)
    }
    
    headers = {
        "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36",
        "Referer": f"https://www.pixiv.net/tags/{encoded_tag}/artworks",
        "Accept-Language": "zh-CN,zh;q=0.9,en;q=0.8",
        "Cookie": PIXIV_COOKIE,
        "Sec-Fetch-Dest": "empty",
        "Sec-Fetch-Mode": "cors",
        "Sec-Fetch-Site": "same-origin",
        "X-Requested-With": "XMLHttpRequest"
    }
    
    try:
        proxy = PROXY if USE_PROXY else None
        async with aiohttp.ClientSession() as session:
            async with session.get(url, headers=headers, params=params, proxy=proxy, timeout=30) as response:
                if response.status != 200:
                    error_text = await response.text()
                    try:
                        error_json = json.loads(error_text)
                        error_msg = error_json.get("error", {}).get("message", error_text[:200])
                    except:
                        error_msg = error_text[:200]
                    raise Exception(f"æœç´¢APIå¤±è´¥ï¼ŒçŠ¶æ€ç : {response.status}, è¯¦æƒ…: {error_msg}")
                
                data = await response.json()
                if not data.get("body") or not data["body"].get("illustManga"):
                    raise Exception("APIè¿”å›ç©ºæ•°æ®ï¼Œå¯èƒ½æ ‡ç­¾æ— æ•ˆæˆ–Cookieå¤±æ•ˆ")
                
                # è·å–æ•´é¡µä½œå“ï¼ˆ60å¼ ï¼‰
                all_results = [
                    item for item in data["body"]["illustManga"]["data"]
                    if item and isinstance(item, dict) and "id" in item and item.get("isAdContainer", 0) == 0
                ]
                
                # ===== R-18å†…å®¹è¿‡æ»¤ =====
                filtered_results = []
                for item in all_results:
                    # è·å–ä½œå“æ ‡ç­¾ï¼ˆå®‰å…¨è®¿é—®ï¼‰
                    tags_info = item.get("tags", [])
                    if isinstance(tags_info, dict):
                        tags_info = tags_info.get("tags", [])
                    
                    # æå–æ ‡ç­¾åç§°
                    tag_names = [tag.get("tag", "").lower() for tag in tags_info if isinstance(tag, dict)]
                    
                    # æ£€æŸ¥R-18/R-18Gæ ‡ç­¾
                    is_r18 = any("r-18" in tag or "r18" in tag for tag in tag_names)
                    is_r18g = any("r-18g" in tag or "r18g" in tag for tag in tag_names)
                    
                    # ä»…ä¿ç•™ç¬¦åˆæ¡ä»¶çš„ä½œå“
                    if is_explicit_r18_request or (not is_r18 and not is_r18g):
                        filtered_results.append(item)
                
                # ç»“æœä¸è¶³æ—¶çš„å¤„ç†
                if not filtered_results:
                    if not is_explicit_r18_request:
                        raise Exception("æœªæ‰¾åˆ°é€‚åˆçš„å†…å®¹ã€‚å¦‚æœæ‚¨æƒ³æœç´¢æˆäººå†…å®¹ï¼Œè¯·åœ¨æ ‡ç­¾ä¸­åŒ…å«'R-18'æˆ–'R-18G'")
                    else:
                        raise Exception("æœªæ‰¾åˆ°åŒ¹é…çš„ä½œå“ï¼Œè¯·å°è¯•å…¶ä»–æ ‡ç­¾æˆ–æ£€æŸ¥Cookieæ˜¯å¦æœ‰æ•ˆ")
                
                # ===== å…³é”®ä¿®æ”¹4ï¼šé‡æ„é€‰æ‹©é€»è¾‘ï¼ˆè´¨é‡+æ–°é²œåº¦ç»¼åˆè¯„åˆ†ï¼‰=====
                scored_candidates = []
                current_time = datetime.now(timezone.utc)
                for item in filtered_results:
                    # è·å–ä½œå“è´¨é‡æŒ‡æ ‡
                    bookmark_count = item.get("bookmarkCount", 0)  # æ”¶è—æ•°
                    like_count = item.get("likeCount", 0)  # ç‚¹èµæ•°
                    view_count = item.get("viewCount", 0)  # æµè§ˆæ•°
                    
                    # è®¡ç®—åŸºç¡€è´¨é‡åˆ†æ•°ï¼ˆé™ä½æƒé‡æ”¾å¤§æ•ˆåº”ï¼‰
                    quality_score = (bookmark_count * 3 + like_count * 2 + view_count * 0.05)
                    
                    # æ·»åŠ æ–°é²œåº¦å› å­ï¼ˆè¿‘7å¤©ä½œå“ä¼˜å…ˆï¼‰
                    create_date = item.get("createDate", "")
                    if create_date:
                        try:
                            # å¤„ç†ä¸åŒæ ¼å¼çš„æ—¥æœŸ
                            if "T" in create_date:
                                create_time = datetime.strptime(create_date.split("T")[0], "%Y-%m-%d").replace(tzinfo=timezone.utc)
                            else:
                                create_time = datetime.strptime(create_date, "%Y-%m-%d").replace(tzinfo=timezone.utc)
                            days_old = (current_time - create_time).days  # 7å¤©å†…æ–°ä½œå“æœ‰åŠ æˆï¼Œè¶Šæ–°æƒé‡è¶Šé«˜
                            freshness_factor = max(0.5, 1 - (days_old / 7))
                            quality_score *= freshness_factor
                        except Exception as date_error:
                            logger.debug(f"æ—¥æœŸè§£æå¤±è´¥: {date_error}")
                    
                    scored_candidates.append((quality_score, item))
                
                # æŒ‰è´¨é‡åˆ†æ•°æ’åºï¼ˆé™åºï¼‰
                scored_candidates.sort(key=lambda x: x[0], reverse=True)
                
                # ä»…å–å‰50ä¸ªé«˜è´¨é‡ä½œå“
                candidates = [item for _, item in scored_candidates[:50]]
                if not candidates:
                    raise Exception("æœªæ‰¾åˆ°æœ‰æ•ˆä½œå“ï¼Œè¯·å°è¯•å…¶ä»–æ ‡ç­¾æˆ–æ£€æŸ¥Cookieæ˜¯å¦æœ‰æ•ˆ")
                
                # ===== å…³é”®ä¿®æ”¹5ï¼šæ™ºèƒ½é€‰æ‹©ç­–ç•¥ï¼ˆ70%é«˜è´¨é‡/30%éšæœºï¼‰=====
                if random.random() < 0.7:
                    # 70%æ¦‚ç‡ä»é«˜è´¨é‡åŒºé€‰æ‹©ï¼ˆå‰30%ï¼‰
                    high_quality_pool = candidates[:max(1, len(candidates) // 3)]
                    selected = random.choice(high_quality_pool)
                else:
                    # 30%æ¦‚ç‡å®Œå…¨éšæœºï¼ˆä¿è¯å¤šæ ·æ€§ï¼‰
                    selected = random.choice(candidates)
                
                illust_id = selected["id"]
                
                # ===== æ–°å¢ï¼šè¿‘æœŸå›¾ç‰‡æ’é™¤æœºåˆ¶ =====
                current_timestamp = time.time()
                # æ¸…ç†è¿‡æœŸç¼“å­˜
                for pid, timestamp in list(RECENT_IMAGES.items()):
                    if current_timestamp - timestamp > EXCLUDE_DURATION:
                        del RECENT_IMAGES[pid]
                
                # æ£€æŸ¥æ˜¯å¦è¿‘æœŸä½¿ç”¨è¿‡
                retry_count = 0
                while str(illust_id) in RECENT_IMAGES and retry_count < 5:
                    retry_count += 1
                    if len(candidates) <= 1:
                        break
                    
                    # ä»æœªä½¿ç”¨è¿‡çš„ä½œå“ä¸­é‡æ–°é€‰æ‹©
                    unused_candidates = [item for item in candidates if str(item["id"]) not in RECENT_IMAGES]
                    if unused_candidates:
                        selected = random.choice(unused_candidates)
                        illust_id = selected["id"]
                    else:
                        # æ‰€æœ‰å€™é€‰ä½œå“è¿‘æœŸéƒ½ç”¨è¿‡ï¼Œé€‰æ‹©æœ€ä¹…æœªç”¨çš„
                        oldest_pid = min(RECENT_IMAGES.items(), key=lambda x: x[1])[0]
                        selected = next((item for item in candidates if str(item["id"]) == oldest_pid), selected)
                        illust_id = selected["id"]
                    break
                
                # è®°å½•å½“å‰ä½¿ç”¨çš„ä½œå“
                RECENT_IMAGES[str(illust_id)] = current_timestamp
                
                # 2. è·å–ä½œå“è¯¦æƒ…
                illust_url = f"https://www.pixiv.net/ajax/illust/{illust_id}"
                illust_headers = {
                    **headers,
                    "Referer": f"https://www.pixiv.net/artworks/{illust_id}"
                }
                
                async with session.get(illust_url, headers=illust_headers, proxy=proxy, timeout=30) as illust_response:
                    if illust_response.status != 200:
                        error_text = await illust_response.text()
                        raise Exception(f"è·å–ä½œå“è¯¦æƒ…å¤±è´¥ï¼ŒçŠ¶æ€ç : {illust_response.status}, å“åº”: {error_text[:200]}")
                    
                    illust_data = await illust_response.json()
                    if illust_data.get("error"):
                        raise Exception(f"ä½œå“è¯¦æƒ…APIé”™è¯¯: {illust_data['message']}")
                    
                    illust_body = illust_data["body"]
                    
                    # å†æ¬¡æ£€æŸ¥R-18æ ‡ç­¾
                    work_tags = [tag.get("tag", "") for tag in illust_body.get("tags", {}).get("tags", [])]
                    is_work_r18 = any(tag.lower() in ["r-18", "r18"] for tag in work_tags)
                    is_work_r18g = any(tag.lower() in ["r-18g", "r18g"] for tag in work_tags)
                    
                    # å¦‚æœä¸æ˜¯æ˜ç¡®è¯·æ±‚R-18ä¸”ä½œå“åŒ…å«R-18æ ‡ç­¾ï¼Œé‡æ–°æœç´¢
                    if not is_explicit_r18_request and (is_work_r18 or is_work_r18g):
                        logger.warning(f"æ£€æµ‹åˆ°R-18å†…å®¹ä½†æœªæ˜ç¡®è¯·æ±‚ï¼Œè·³è¿‡ä½œå“ID: {illust_id}")
                        # ä¸ºé¿å…æ— é™é€’å½’ï¼Œä¸åœ¨æ­¤å¤„é€’å½’ï¼Œè€Œæ˜¯æŠ›å‡ºå¼‚å¸¸è®©ç”¨æˆ·é‡è¯•
                        raise Exception("æ£€æµ‹åˆ°ä¸é€‚å½“å†…å®¹ï¼Œå·²è·³è¿‡ã€‚è¯·å°è¯•å…¶ä»–æ ‡ç­¾ã€‚")
                    
                    original_img_url = illust_body["urls"]["original"]
                    regular_img_url = illust_body["urls"]["regular"]
                    
                    # æ„å»ºä»£ç†åçš„å›¾ç‰‡URL
                    proxy_original_url = replace_image_domain(original_img_url)
                    proxy_preview_url = replace_image_domain(regular_img_url)
                    
                    return {
                        "image_url": proxy_original_url,
                        "pid": str(illust_id),
                        "title": illust_body["title"],
                        "author": illust_body["userName"],
                        "author_id": illust_body["userId"],
                        "work_url": f"https://www.pixiv.net/artworks/{illust_id}",
                        "preview_url": proxy_preview_url,
                        "original_url": original_img_url,
                        "stats": {
                            "bookmarks": selected.get("bookmarkCount", 0),
                            "likes": selected.get("likeCount", 0),
                            "views": selected.get("viewCount", 0)
                        }
                    }
    except Exception as e:
        raise Exception(f"æœç´¢å¤±è´¥: {str(e)}")

def replace_image_domain(url: str) -> str:
    """å°†Pixivå›¾ç‰‡åŸŸåæ›¿æ¢ä¸ºä»£ç†åŸŸåï¼Œå¹¶ç¡®ä¿æ–‡ä»¶æ ¼å¼å…¼å®¹"""
    if not url.startswith("http"):
        url = "https:" + url
    
    proxy_base = PROXY_URL.rstrip('/') + '/'
    
    # ä¿®å¤URLä¸­çš„è½¬ä¹‰å­—ç¬¦
    url = url.replace("%2F", "/").replace("%3A", ":")
    
    if "i.pximg.net" in url:
        url = url.replace("https://i.pximg.net", proxy_base.rstrip('/'))
    elif "pixiv.cat" in url:
        url = url.replace("https://pixiv.cat", proxy_base.rstrip('/'))
    
    # ç¡®ä¿æ–‡ä»¶æ ¼å¼å…¼å®¹ï¼ˆé¿å…WebPç­‰ä¸æ”¯æŒçš„æ ¼å¼ï¼‰
    if url.endswith('.webp'):
        url = url[:-5] + '.jpg'  # è½¬ä¸º jpg
    elif url.endswith('.gif') and 'ugoira' not in url:  # éåŠ¨å›¾GIFè½¬ä¸ºJPG
        url = url[:-4] + '.jpg'
    
    # æ›¿æ¢URLä¸­çš„ç‰¹æ®Šå­—ç¬¦ï¼ˆé˜²æ­¢è·¯å¾„é—®é¢˜ï¼‰
    url = url.replace(' ', '%20').replace('&', '%26').replace('?', '%3F')
    return url

# ====== åŸå›¾ä¸“ç”¨å¤„ç†å‡½æ•° ======
async def get_remote_file_size(url: str) -> int:
    """è·å–è¿œç¨‹æ–‡ä»¶å¤§å°ï¼Œé¿å…ä¸‹è½½å¤§æ–‡ä»¶"""
    try:
        proxy = PROXY if USE_PROXY else None
        headers = {
            "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36",
            "Referer": "https://www.pixiv.net/"
        }
        
        async with aiohttp.ClientSession() as session:
            async with session.head(
                url, headers=headers, proxy=proxy, timeout=aiohttp.ClientTimeout(total=10)
            ) as response:
                if response.status in (200, 206):
                    content_range = response.headers.get('Content-Range', '')
                    if content_range:
                        # ä»Content-Rangeä¸­æå–æ–‡ä»¶å¤§å°ï¼šbytes 0-0/12345678
                        return int(content_range.split('/')[-1])
                    
                    content_length = response.headers.get('Content-Length')
                    if content_length:
                        return int(content_length)
                    else:
                        # å°è¯•GETè¯·æ±‚å‰1KB
                        headers['Range'] = 'bytes=0-1023'
                        async with session.get(
                            url, headers=headers, proxy=proxy, timeout=aiohttp.ClientTimeout(total=10)
                        ) as response:
                            if response.status in (200, 206):
                                content_length = response.headers.get('Content-Length')
                                if content_length:
                                    # ä¼°ç®—å®Œæ•´æ–‡ä»¶å¤§å°ï¼ˆ1024å­—èŠ‚æ˜¯å¤´éƒ¨ï¼Œæ€»å¤§å°é€šå¸¸å¤§äºå¤´éƒ¨ï¼‰
                                    estimated_size = int(content_length)
                                    return estimated_size * 10  # ç²—ç•¥ä¼°è®¡
                
                return 0
    except Exception as e:
        logger.warning(f"è·å–æ–‡ä»¶å¤§å°å¤±è´¥: {str(e)}")
        return 0

# ====== æ·»åŠ å›¾ç‰‡å‹ç¼©åŠŸèƒ½ ======
try:
    from PIL import Image
    import io
    PILLLOW_AVAILABLE = True
except ImportError:
    PILLLOW_AVAILABLE = False

async def compress_image(file_path: Path, max_size: int = 10 * 1024 * 1024) -> Path:
    """å‹ç¼©å›¾ç‰‡ï¼Œç¡®ä¿ä¸è¶…è¿‡æŒ‡å®šå¤§å°ï¼ˆ10MBï¼‰"""
    if not PILLLOW_AVAILABLE:
        logger.warning("Pillowåº“æœªå®‰è£…ï¼Œæ— æ³•å‹ç¼©å›¾ç‰‡")
        return None
    
    try:
        # è¯»å–å›¾ç‰‡
        with Image.open(file_path) as img:
            # è·å–åŸå§‹å°ºå¯¸
            width, height = img.size
            
            # å¦‚æœå›¾ç‰‡å·²ç»å°äº10MBï¼Œç›´æ¥è¿”å›
            if file_path.stat().st_size <= max_size:
                return file_path
            
            # å°è¯•å‹ç¼©å›¾ç‰‡
            quality = 95
            while quality > 50 and file_path.stat().st_size > max_size:
                # ä¿å­˜å‹ç¼©åçš„å›¾ç‰‡
                buffer = io.BytesIO()
                img.save(buffer, format="JPEG", quality=quality, optimize=True)
                buffer.seek(0)
                compressed_size = buffer.tell()
                
                # å¦‚æœå‹ç¼©åçš„å¤§å°ç¬¦åˆè¦æ±‚ï¼Œä¿å­˜å¹¶è¿”å›
                if compressed_size <= max_size:
                    new_file_path = file_path.with_suffix('.jpg')
                    with open(new_file_path, 'wb') as f:
                        f.write(buffer.read())
                    return new_file_path
                
                quality -= 5
            
            # å¦‚æœå‹ç¼©åˆ°æœ€ä½è´¨é‡ä»ç„¶å¤ªå¤§ï¼Œä½¿ç”¨é¢„è§ˆå›¾
            return None
    except Exception as e:
        logger.error(f"å›¾ç‰‡å‹ç¼©å¤±è´¥: {str(e)}")
        return None

async def download_original_image(url: str) -> Path:
    """å®‰å…¨ä¸‹è½½å¤§æ–‡ä»¶åˆ°ä¸´æ—¶ä½ç½®ï¼Œè¿”å›æ–‡ä»¶è·¯å¾„ï¼ˆç¡®ä¿ä¸è¶…è¿‡10MBï¼‰"""
    file_size = await get_remote_file_size(url)
    
    # ç”Ÿæˆå”¯ä¸€æ–‡ä»¶å
    timestamp = int(time.time() * 1000)
    random_str = ''.join(random.choices('abcdefghijklmnopqrstuvwxyz0123456789', k=8))
    parsed_url = urllib.parse.urlparse(url)
    ext = os.path.splitext(parsed_url.path)[1] or '.jpg'
    
    # ç¡®ä¿æ–‡ä»¶æ‰©å±•åå…¼å®¹
    ext = ext.lower()
    if ext in ['.webp', '.avif', '.heic']:
        ext = '.jpg'
    elif ext == '.svg':
        ext = '.png'
    
    filename = f"pixiv_{timestamp}_{random_str}{ext}"
    temp_path = TEMP_DIR / filename
    
    logger.info(f"å¼€å§‹ä¸‹è½½åŸå›¾åˆ°: {temp_path} (é¢„ä¼°å¤§å°: {file_size/1024/1024:.2f}MB)")
    
    proxy = PROXY if USE_PROXY else None
    headers = {
        "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36",
        "Referer": "https://www.pixiv.net/"
    }
    
    # åˆ›å»ºSSLä¸Šä¸‹æ–‡ï¼ˆé¿å…SSLéªŒè¯é—®é¢˜ï¼‰
    ssl_context = ssl.create_default_context()
    ssl_context.check_hostname = False
    ssl_context.verify_mode = ssl.CERT_NONE
    
    # é‡è¯•æœºåˆ¶
    for attempt in range(MAX_ATTEMPTS):
        try:
            async with aiohttp.ClientSession() as session:
                async with session.get(
                    url, headers=headers, proxy=proxy, timeout=aiohttp.ClientTimeout(total=DOWNLOAD_TIMEOUT), ssl=ssl_context
                ) as response:
                    if response.status != 200:
                        error_text = await response.text()
                        raise Exception(f"ä¸‹è½½å¤±è´¥ï¼ŒçŠ¶æ€ç : {response.status}, å“åº”: {error_text[:200]}")
                    
                    # åˆ†å—å†™å…¥æ–‡ä»¶ï¼Œé¿å…å†…å­˜æº¢å‡º
                    total_bytes = 0
                    start_time = time.time()
                    async with aiofiles.open(temp_path, 'wb') as f:
                        async for chunk in response.content.iter_chunked(MAX_DOWNLOAD_CHUNK):
                            await f.write(chunk)
                            total_bytes += len(chunk)
                    
                    # éªŒè¯æ–‡ä»¶å®Œæ•´æ€§
                    downloaded_size = temp_path.stat().st_size
                    if file_size > 0 and downloaded_size < file_size * 0.9:
                        raise Exception(f"æ–‡ä»¶ä¸å®Œæ•´: æœŸæœ› {file_size} å­—èŠ‚, å®é™… {downloaded_size} å­—èŠ‚")
                    
                    # éªŒè¯å›¾ç‰‡æœ‰æ•ˆæ€§ï¼ˆéœ€è¦Pillowï¼‰
                    try:
                        if PILLLOW_AVAILABLE:
                            with Image.open(temp_path) as img:
                                img.verify()  # éªŒè¯æ˜¯å¦ä¸ºæœ‰æ•ˆçš„å›¾ç‰‡æ ¼å¼
                    except ImportError:
                        logger.warning("æœªå®‰è£…Pillowåº“ï¼Œè·³è¿‡å›¾ç‰‡éªŒè¯ã€‚å»ºè®®å®‰è£…: pip install Pillow")
                    except Exception as e:
                        logger.warning(f"å›¾ç‰‡éªŒè¯å¤±è´¥ï¼Œå°è¯•ä¿®å¤: {str(e)}")
                        # å°è¯•ä¿®å¤ï¼šé‡å‘½åæ‰©å±•å
                        if not str(temp_path).endswith(('.jpg', '.jpeg', '.png')):
                            new_path = temp_path.with_suffix('.jpg')
                            temp_path.rename(new_path)
                            temp_path = new_path
                    
                    # æ£€æŸ¥æ–‡ä»¶å¤§å°å¹¶å‹ç¼©ï¼ˆå¦‚æœéœ€è¦ï¼‰
                    if downloaded_size > 10 * 1024 * 1024:  # è¶…è¿‡10MB
                        logger.warning(f"âš ï¸ å›¾ç‰‡è¿‡å¤§ ({downloaded_size/1024/1024:.1f}MB)ï¼Œå°è¯•å‹ç¼©...")
                        compressed_path = await compress_image(temp_path)
                        if compressed_path:
                            temp_path = compressed_path
                            logger.info(f"âœ… å›¾ç‰‡å·²å‹ç¼©è‡³ {temp_path.stat().st_size/1024/1024:.2f}MB")
                        else:
                            logger.warning("âš ï¸ å›¾ç‰‡å‹ç¼©å¤±è´¥ï¼Œå°†ä½¿ç”¨é¢„è§ˆå›¾")
                            return None  # è¿”å›Noneè¡¨ç¤ºéœ€è¦ä½¿ç”¨é¢„è§ˆå›¾
                    
                    logger.info(f"âœ… åŸå›¾ä¸‹è½½æˆåŠŸ: {downloaded_size/1024/1024:.2f}MB, è€—æ—¶: {time.time()-start_time:.1f}s")
                    return temp_path
        except Exception as e:
            logger.error(f"ä¸‹è½½å°è¯• {attempt+1}/{MAX_ATTEMPTS} å¤±è´¥: {str(e)}")
            if attempt == MAX_ATTEMPTS - 1:
                raise
            await asyncio.sleep(2)
    
    return temp_path  # å¦‚æœæ²¡æœ‰è¿”å›ï¼Œè¿”å›ä¸´æ—¶è·¯å¾„

async def cleanup_temp_files():
    """æ¸…ç†12å°æ—¶ä»¥ä¸Šçš„ä¸´æ—¶æ–‡ä»¶"""
    try:
        now = time.time()
        for file_path in TEMP_DIR.glob("*"):
            if file_path.is_file():
                file_age = now - file_path.stat().st_mtime
                if file_age > 12 * 3600:  # 12å°æ—¶
                    try:
                        file_path.unlink()
                        logger.debug(f"æ¸…ç†æ—§ä¸´æ—¶æ–‡ä»¶: {file_path.name}")
                    except Exception as e:
                        logger.warning(f"æ¸…ç†æ–‡ä»¶å¤±è´¥ {file_path.name}: {str(e)}")
    except Exception as e:
        logger.warning(f"æ¸…ç†ä¸´æ—¶æ–‡ä»¶æ—¶å‡ºé”™: {str(e)}")

# ====== Nonebot2æ’ä»¶é€»è¾‘ ======
pixiv_cmd = on_command("pixiv", aliases={"p"}, priority=5, block=True)

@pixiv_cmd.handle()
async def handle_pixiv_command(bot: Bot, event: Event):
    """å¤„ç† /pixiv å‘½ä»¤ - åŸå›¾ä¼˜å…ˆæ¨¡å¼"""
    # ===== æ–°å¢ï¼šå†·å´æœºåˆ¶æ£€æŸ¥ =====
    user_id = event.get_user_id()
    current_time = time.time()
    
    # æ£€æŸ¥æ˜¯å¦åœ¨å†·å´ä¸­
    if user_id in last_request_time:
        elapsed = current_time - last_request_time[user_id]
        if elapsed < COOLDOWN_TIME:
            remaining = COOLDOWN_TIME - elapsed
            await bot.send(event, f"è¯·æ±‚è¿‡äºé¢‘ç¹ï¼Œè¯·ç­‰å¾… {remaining:.1f} ç§’åå†è¯•")
            return
    
    # æ›´æ–°æœ€åè¯·æ±‚æ—¶é—´
    last_request_time[user_id] = current_time
    
    raw_message = str(event.get_message()).strip()
    command_str = event.get_plaintext().split()[0]
    args = raw_message[len(command_str):].strip()
    
    if not args:
        await bot.send(event, "è¯·æä¾›æœç´¢æ ‡ç­¾ï¼Œä¾‹å¦‚ï¼š\n/pixiv é¸£æ½®\n/p é¸£æ½®")
        return
    
    tags = [tag.strip() for tag in args.split() if tag.strip()]
    logger.info(f"Pixivæœç´¢è¯·æ±‚: {tags}")
    
    try:
        # 1. æœç´¢ä½œå“
        result = await search_pixiv_by_tag(tags)
        
        # 2. æ„å»ºæ¶ˆæ¯å†…å®¹
        msg_content = (
            f"ğŸ¨ ä½œå“æ ‡é¢˜: {result['title']}\n"
            f"ğŸ‘¤ ä½œè€…: {result['author']} (ID: {result['author_id']})\n"
            f"ğŸ†” ä½œå“ID: {result['pid']}\n"
            f"ğŸ”— ä½œå“é“¾æ¥: {result['work_url']}\n\n"
            f"â³ æ­£åœ¨ä¸‹è½½åŸå›¾ (å¯èƒ½éœ€è¦è¾ƒé•¿æ—¶é—´)..."
        )
        
        # å‘é€åˆæ­¥ä¿¡æ¯
        await bot.send(event, msg_content)
        
        # 3. å®‰å…¨ä¸‹è½½åŸå›¾
        try:
            # æ¸…ç†æ—§ä¸´æ—¶æ–‡ä»¶
            await cleanup_temp_files()
            
            # ä¸‹è½½åŸå›¾
            file_path = await download_original_image(result['image_url'])
            
            # æ£€æŸ¥æ–‡ä»¶æ˜¯å¦å­˜åœ¨
            if not file_path or not file_path.exists():
                if file_path is None:
                    logger.warning("âš ï¸ åŸå›¾å‹ç¼©å¤±è´¥ï¼Œå°†ä½¿ç”¨é¢„è§ˆå›¾")
                else:
                    raise FileNotFoundError(f"æ–‡ä»¶ä¸å­˜åœ¨: {file_path}")
                
                # é™çº§å‘é€é¢„è§ˆå›¾
                fallback_msg = (
                    f"âš ï¸ åŸå›¾è¿‡å¤§æˆ–å‹ç¼©å¤±è´¥ï¼Œå·²è‡ªåŠ¨é™çº§ä¸ºé¢„è§ˆå›¾\n"
                    f"ğŸ”— åŸå›¾ä¸‹è½½: {result['image_url']}\n\n"
                    f"ğŸ–¼ï¸ å½“å‰æ˜¾ç¤ºé¢„è§ˆå›¾ï¼ˆç‚¹å‡»é“¾æ¥ä¸‹è½½åŸå›¾ï¼‰:"
                )
                await bot.send(event, fallback_msg)
                
                # å‘é€é¢„è§ˆå›¾
                preview_data = await download_and_process_preview(result['preview_url'])
                await bot.send(event, MessageSegment.image(preview_data))
                return
            
            # æ£€æŸ¥æ–‡ä»¶å¤§å°
            file_size = file_path.stat().st_size
            if file_size > 10 * 1024 * 1024:  # è¶…è¿‡10MB
                logger.warning(f"âš ï¸ å›¾ç‰‡è¿‡å¤§ ({file_size/1024/1024:.1f}MB)ï¼Œå·²è‡ªåŠ¨é™çº§ä¸ºé¢„è§ˆå›¾")
                fallback_msg = (
                    f"âš ï¸ åŸå›¾è¿‡å¤§ï¼ˆ{file_size/1024/1024:.1f}MBï¼‰ï¼Œå·²è‡ªåŠ¨é™çº§ä¸ºé¢„è§ˆå›¾\n"
                    f"ğŸ”— åŸå›¾ä¸‹è½½: {result['image_url']}\n\n"
                    f"ğŸ–¼ï¸ å½“å‰æ˜¾ç¤ºé¢„è§ˆå›¾ï¼ˆç‚¹å‡»é“¾æ¥ä¸‹è½½åŸå›¾ï¼‰:"
                )
                await bot.send(event, fallback_msg)
                
                # å‘é€é¢„è§ˆå›¾
                preview_data = await download_and_process_preview(result['preview_url'])
                await bot.send(event, MessageSegment.image(preview_data))
                return
            
            # å‘é€åŸå›¾
            logger.info(f"å‡†å¤‡å‘é€æ–‡ä»¶è·¯å¾„: {file_path}")
            
            start_time = time.time()
            # è¯»å–æ–‡ä»¶å†…å®¹
            try:
                async with aiofiles.open(file_path, 'rb') as f:
                    image_data = await f.read()
                await bot.send(event, MessageSegment.image(image_data))
                logger.info(f"âœ… åŸå›¾å‘é€æˆåŠŸ! è€—æ—¶: {time.time()-start_time:.1f}s")
            except Exception as e:
                logger.error(f"å‘é€å¤±è´¥: {str(e)}")
                raise e
            # 4. åŒæ­¥æ¸…ç†æ–‡ä»¶ï¼ˆç¡®ä¿å‘é€å®Œæˆåå†åˆ é™¤ï¼‰
            try:
                # ç­‰å¾…ä¸€å°æ®µæ—¶é—´ç¡®ä¿æ¶ˆæ¯å®Œå…¨å‘é€
                await asyncio.sleep(1)
                if file_path.exists():
                    file_path.unlink()
                    logger.debug(f"âœ… å·²æ¸…ç†ä¸´æ—¶æ–‡ä»¶: {file_path}")
            except Exception as e:
                logger.warning(f"æ¸…ç†æ–‡ä»¶è­¦å‘Š {file_path}: {str(e)}")
        
        except Exception as e:
            error_msg = str(e)
            logger.error(f"åŸå›¾å‘é€å¤±è´¥: {error_msg}\n{traceback.format_exc()}")
            
            # é™çº§æ–¹æ¡ˆï¼šå‘é€é¢„è§ˆå›¾ + åŸå›¾é“¾æ¥
            fallback_msg = (
                f"âš ï¸ åŸå›¾å‘é€å¤±è´¥ï¼ˆå¯èƒ½æ–‡ä»¶è¿‡å¤§æˆ–ç½‘ç»œé—®é¢˜ï¼‰ï¼Œå·²è‡ªåŠ¨é™çº§\n"
                f"ğŸ”— åŸå›¾ä¸‹è½½: {result['image_url']}\n\n"
                f"ğŸ–¼ï¸ å½“å‰æ˜¾ç¤ºé¢„è§ˆå›¾ï¼ˆç‚¹å‡»é“¾æ¥ä¸‹è½½åŸå›¾ï¼‰:"
            )
            await bot.send(event, fallback_msg)
            
            # å‘é€é¢„è§ˆå›¾
            preview_data = await download_and_process_preview(result['preview_url'])
            await bot.send(event, MessageSegment.image(preview_data))
    
    except Exception as e:
        error_msg = str(e)
        logger.error(f"Pixivæœç´¢å¤±è´¥: {error_msg}\n{traceback.format_exc()}")
        
        # ä¼˜åŒ–é”™è¯¯æç¤º
        if "Cookie" in error_msg or "cookie" in error_msg.lower():
            error_msg = (
                "âš ï¸ Cookieæ— æ•ˆï¼è¯·é‡æ–°è·å–Pixiv Cookie:\n"
                "1. ç™»å½• https://www.pixiv.net\n"
                "2. æŒ‰ F12 æ‰“å¼€å¼€å‘è€…å·¥å…·\n"
                "3. è¿›å…¥ Application â†’ Storage â†’ Cookies\n"
                "4. å¤åˆ¶æ•´ä¸ª Cookie å†…å®¹"
            )
        elif "ä»£ç†" in error_msg or "proxy" in error_msg.lower() or "Proxy" in error_msg:
            error_msg = (
                f"âš ï¸ ä»£ç†é…ç½®é—®é¢˜ï¼è¯·æ£€æŸ¥:\n"
                f"- æœ¬åœ°ä»£ç†: {PROXY}\n"
                f"- Cloudflare ä»£ç†: {PROXY_URL}\n"
                "- ç¡®ä¿ä»£ç†è½¯ä»¶æ­£å¸¸è¿è¡Œ"
            )
        elif "timeout" in error_msg.lower() or "è¶…æ—¶" in error_msg:
            error_msg = (
                "âš ï¸ è¯·æ±‚è¶…æ—¶ï¼å¯èƒ½æ˜¯ç½‘ç»œä¸ç¨³å®šæˆ–ä»£ç†å»¶è¿Ÿè¿‡é«˜\n"
                "å»ºè®®:\n"
                "1. æ£€æŸ¥ä»£ç†æ˜¯å¦æ­£å¸¸è¿è¡Œ\n"
                "2. å°è¯•æ›´æ¢æ ‡ç­¾\n"
                "3. æ£€æŸ¥Cloudflare Workersæ˜¯å¦å¯ç”¨"
            )
        elif "memory access out of bounds" in error_msg or "å†…å­˜" in error_msg:
            error_msg = (
                "âš ï¸ å†…å­˜æº¢å‡ºï¼åŸå›¾è¿‡å¤§å¯¼è‡´\n"
                "å·²è‡ªåŠ¨é™çº§å‘é€é¢„è§ˆå›¾\n"
                "æ‚¨ä¹Ÿå¯ä»¥é€šè¿‡ä½œå“é“¾æ¥ä¸‹è½½åŸå›¾"
            )
        elif "404" in error_msg or "403" in error_msg:
            error_msg = (
                "âš ï¸ æ— æ³•è®¿é—®å›¾ç‰‡èµ„æº\n"
                "å¯èƒ½æ˜¯ä»£ç†é…ç½®æœ‰è¯¯æˆ–Pixivé™åˆ¶"
            )
        else:
            error_msg = f"å‘ç”ŸæœªçŸ¥é”™è¯¯: {error_msg}"
        
        await bot.send(event, f"âŒ æœç´¢å¤±è´¥: {error_msg}")

# ====== é¢„è§ˆå›¾å¤„ç†å‡½æ•°ï¼ˆé™çº§ç”¨ï¼‰ ======
async def download_and_process_preview(image_url: str) -> bytes:
    """ä¸‹è½½å¹¶å¤„ç†é¢„è§ˆå›¾ï¼ˆå°å°ºå¯¸ï¼‰"""
    try:
        proxy = PROXY if USE_PROXY else None
        headers = {
            "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36",
            "Referer": "https://www.pixiv.net/"
        }
        
        async with aiohttp.ClientSession() as session:
            async with session.get(
                image_url, headers=headers, proxy=proxy, timeout=aiohttp.ClientTimeout(total=15)
            ) as response:
                if response.status != 200:
                    raise Exception(f"é¢„è§ˆå›¾ä¸‹è½½å¤±è´¥ï¼ŒçŠ¶æ€ç : {response.status}")
                return await response.read()
    except Exception as e:
        logger.error(f"é¢„è§ˆå›¾å¤„ç†å¤±è´¥: {str(e)}")
        raise Exception(f"é¢„è§ˆå›¾å¤„ç†å¤±è´¥: {str(e)}")