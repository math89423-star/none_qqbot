import aiohttp
import json
import urllib.parse
import random
import time
import os
import tempfile
import aiofiles
from nonebot import on_command, logger
from nonebot.adapters.onebot.v11 import MessageSegment, Bot, Event
from typing import Dict, Any
import asyncio
import ssl
import traceback
from pathlib import Path

# ====== é‡è¦é…ç½®ï¼ˆå¿…é¡»ä¿®æ”¹ï¼‰ ======
PROXY = "http://127.0.0.1:7890"  # æœ¬åœ°ä»£ç†åœ°å€
USE_PROXY = True

PROXY_URL = "https://quiet-hill-31f3.math89423.workers.dev/"  # Cloudflare Workersåœ°å€

PIXIV_COOKIE = "PHPSESSID=14916444_EuNtNE3Yd2ZZ50A7UzivUlxP7O2hLP7s; device_token=ccd49454e972c3b547f1db56a3560575; p_ab_id=1; p_ab_id_2=1"  # â† å¿…é¡»ä¿®æ”¹ï¼

# åŸå›¾å‘é€ä¸“ç”¨é…ç½®
MAX_DOWNLOAD_CHUNK = 8192  # 8KBåˆ†å—ä¸‹è½½
DOWNLOAD_TIMEOUT = 60  # 60ç§’è¶…æ—¶
MAX_ATTEMPTS = 2  # é‡è¯•æ¬¡æ•°
TEMP_DIR = tempfile.gettempdir()  # ç³»ç»Ÿä¸´æ—¶ç›®å½•

# åˆ›å»ºä¸“ç”¨ä¸´æ—¶ç›®å½•
os.makedirs(os.path.join(TEMP_DIR, "pixiv_bot"), exist_ok=True)

# ====== æ ¸å¿ƒå‡½æ•° ======
async def search_pixiv_by_tag(tags: list, max_results=10) -> dict:
    """
    é€šè¿‡è§’è‰²æ ‡ç­¾æœç´¢Pixivå›¾ç‰‡ï¼ˆä¼˜åŒ–é‡å¤ç‡ï¼Œæ·»åŠ R-18è¿‡æ»¤ï¼ŒæŒ‰çƒ­åº¦æ’åºä¸”é™åˆ¶è¿‘ä¸€å‘¨ï¼‰
    """
    search_tag = " ".join(tags)
    encoded_tag = urllib.parse.quote(search_tag)
    
    # ===== å…³é”®ä¿®æ”¹1ï¼šæ£€æŸ¥æ˜¯å¦æ˜ç¡®è¯·æ±‚R-18å†…å®¹ =====
    is_explicit_r18_request = any(tag.lower() in ["r-18", "r18", "r-18g", "r18g"] for tag in tags)
    
    # ===== å…³é”®ä¿®æ”¹2ï¼šè®¾ç½®å®‰å…¨æ¨¡å¼å‚æ•° =====
    search_mode = "all" if is_explicit_r18_request else "safe"
    
    # ===== å…³é”®ä¼˜åŒ–ï¼šæŒ‰çƒ­åº¦æ’åº + è¿‘ä¸€å‘¨æ—¶é—´èŒƒå›´ =====
    # è®¡ç®—è¿‘ä¸€å‘¨çš„æ—¥æœŸèŒƒå›´ (æ ¼å¼: YYYY-MM-DD)
    from datetime import datetime, timedelta
    end_date = datetime.now().strftime("%Y-%m-%d")
    start_date = (datetime.now() - timedelta(days=7)).strftime("%Y-%m-%d")
    
    # éšæœºåç§»èµ·å§‹ä½ç½®
    offset = random.randint(10, 50)  # çƒ­é—¨ä½œå“é›†ä¸­åœ¨å‰å‡ é¡µï¼Œå‡å°‘åç§»é‡
    page = offset // 60 + 1
    start_index = offset % 60
    
    url = f"https://www.pixiv.net/ajax/search/artworks/{encoded_tag}"
    params = {
        "word": search_tag,
        "order": "popular_d",  # æ”¹ä¸ºæŒ‰çƒ­åº¦é™åºæ’åˆ—
        "mode": search_mode,   # ä½¿ç”¨å®‰å…¨æ¨¡å¼å‚æ•°
        "p": page,
        "s_mode": "s_tag",
        "type": "all",
        "lang": "zh",
        "scd": start_date,     # å¼€å§‹æ—¥æœŸ (è¿‘ä¸€å‘¨)
        "ecd": end_date,       # ç»“æŸæ—¥æœŸ (ä»Šå¤©)
        "blt": "200"           # æœ€ä½æ”¶è—æ•° (è¿‡æ»¤ä½è´¨é‡ä½œå“)
    }
    
    headers = {
        "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36",
        "Referer": f"https://www.pixiv.net/tags/{encoded_tag}/artworks",
        "Accept-Language": "zh-CN,zh;q=0.9,en;q=0.8",
        "Cookie": PIXIV_COOKIE,
        "Sec-Fetch-Dest": "empty",
        "Sec-Fetch-Mode": "cors",
        "Sec-Fetch-Site": "same-origin",
        "X-Requested-With": "XMLHttpRequest"
    }
    
    try:
        proxy = PROXY if USE_PROXY else None
        
        async with aiohttp.ClientSession() as session:
            async with session.get(url, headers=headers, params=params, proxy=proxy, timeout=30) as response:
                if response.status != 200:
                    error_text = await response.text()
                    try:
                        error_json = json.loads(error_text)
                        error_msg = error_json.get("error", {}).get("message", error_text[:200])
                    except:
                        error_msg = error_text[:200]
                    raise Exception(f"æœç´¢APIå¤±è´¥ï¼ŒçŠ¶æ€ç : {response.status}, è¯¦æƒ…: {error_msg}")
                
                data = await response.json()
                
                if not data.get("body") or not data["body"].get("illustManga"):
                    raise Exception("APIè¿”å›ç©ºæ•°æ®ï¼Œå¯èƒ½æ ‡ç­¾æ— æ•ˆæˆ–Cookieå¤±æ•ˆ")
                
                # è·å–æ•´é¡µä½œå“ï¼ˆ60å¼ ï¼‰
                all_results = [
                    item for item in data["body"]["illustManga"]["data"] 
                    if item and isinstance(item, dict) and "id" in item and item.get("isAdContainer", 0) == 0
                ]
                
                # ===== R-18å†…å®¹è¿‡æ»¤ =====
                filtered_results = []
                for item in all_results:
                    # è·å–ä½œå“æ ‡ç­¾ï¼ˆå®‰å…¨è®¿é—®ï¼‰
                    tags_info = item.get("tags", [])
                    if isinstance(tags_info, dict):
                        tags_info = tags_info.get("tags", [])
                    
                    # æå–æ ‡ç­¾åç§°
                    tag_names = [tag.get("tag", "").lower() for tag in tags_info if isinstance(tag, dict)]
                    
                    # æ£€æŸ¥R-18/R-18Gæ ‡ç­¾
                    is_r18 = any("r-18" in tag or "r18" in tag for tag in tag_names)
                    is_r18g = any("r-18g" in tag or "r18g" in tag for tag in tag_names)
                    
                    # ä»…ä¿ç•™ç¬¦åˆæ¡ä»¶çš„ä½œå“
                    if is_explicit_r18_request or (not is_r18 and not is_r18g):
                        filtered_results.append(item)
                
                # ç»“æœä¸è¶³æ—¶çš„å¤„ç†
                if not filtered_results:
                    if not is_explicit_r18_request:
                        raise Exception("æœªæ‰¾åˆ°é€‚åˆçš„å†…å®¹ã€‚å¦‚æœæ‚¨æƒ³æœç´¢æˆäººå†…å®¹ï¼Œè¯·åœ¨æ ‡ç­¾ä¸­åŒ…å«'R-18'æˆ–'R-18G'")
                    else:
                        raise Exception("æœªæ‰¾åˆ°åŒ¹é…çš„ä½œå“ï¼Œè¯·å°è¯•å…¶ä»–æ ‡ç­¾æˆ–æ£€æŸ¥Cookieæ˜¯å¦æœ‰æ•ˆ")
                
                # é€‰æ‹©å€™é€‰ä½œå“ï¼ˆåªå–å‰30ä¸ªé«˜è´¨é‡ä½œå“ï¼‰
                candidates = filtered_results[:30]
                
                if not candidates:
                    raise Exception("æœªæ‰¾åˆ°æœ‰æ•ˆä½œå“ï¼Œè¯·å°è¯•å…¶ä»–æ ‡ç­¾æˆ–æ£€æŸ¥Cookieæ˜¯å¦æœ‰æ•ˆ")
                
                # ===== ä¼˜åŒ–ï¼šæ ¹æ®ç»¼åˆè´¨é‡è¯„åˆ†åŠ æƒé€‰æ‹© =====
                weighted_candidates = []
                for item in candidates:
                    # è·å–ä½œå“è´¨é‡æŒ‡æ ‡
                    bookmark_count = item.get("bookmarkCount", 0)  # æ”¶è—æ•°
                    like_count = item.get("likeCount", 0)          # ç‚¹èµæ•°
                    view_count = item.get("viewCount", 0)          # æµè§ˆæ•°
                    
                    # è®¡ç®—ç»¼åˆè´¨é‡åˆ†æ•° (æ”¶è—æƒé‡æœ€é«˜ï¼Œå…¶æ¬¡æ˜¯ç‚¹èµ)
                    quality_score = (
                        bookmark_count * 10 + 
                        like_count * 5 + 
                        view_count * 0.1
                    )
                    
                    # ä¿è¯è‡³å°‘1æƒé‡
                    weight = max(1, min(100, int(quality_score ** 0.5)))  # å¼€æ–¹å¹³æ»‘
                    weighted_candidates.extend([item] * weight)
                
                # éšæœºé€‰æ‹©ï¼ˆé«˜åˆ†ä½œå“æ¦‚ç‡æ›´é«˜ï¼‰
                selected = random.choice(weighted_candidates)
                illust_id = selected["id"]
                
                # 2. è·å–ä½œå“è¯¦æƒ…
                illust_url = f"https://www.pixiv.net/ajax/illust/{illust_id}"
                illust_headers = {
                    **headers,
                    "Referer": f"https://www.pixiv.net/artworks/{illust_id}"
                }
                
                async with session.get(illust_url, headers=illust_headers, proxy=proxy, timeout=30) as illust_response:
                    if illust_response.status != 200:
                        error_text = await illust_response.text()
                        raise Exception(f"è·å–ä½œå“è¯¦æƒ…å¤±è´¥ï¼ŒçŠ¶æ€ç : {illust_response.status}, å“åº”: {error_text[:200]}")
                    
                    illust_data = await illust_response.json()
                    if illust_data.get("error"):
                        raise Exception(f"ä½œå“è¯¦æƒ…APIé”™è¯¯: {illust_data['message']}")
                    
                    illust_body = illust_data["body"]
                    
                    # å†æ¬¡æ£€æŸ¥R-18æ ‡ç­¾
                    work_tags = [tag.get("tag", "") for tag in illust_body.get("tags", {}).get("tags", [])]
                    is_work_r18 = any(tag.lower() in ["r-18", "r18"] for tag in work_tags)
                    is_work_r18g = any(tag.lower() in ["r-18g", "r18g"] for tag in work_tags)
                    
                    # å¦‚æœä¸æ˜¯æ˜ç¡®è¯·æ±‚R-18ä¸”ä½œå“åŒ…å«R-18æ ‡ç­¾ï¼Œé‡æ–°æœç´¢
                    if not is_explicit_r18_request and (is_work_r18 or is_work_r18g):
                        logger.warning(f"æ£€æµ‹åˆ°R-18å†…å®¹ä½†æœªæ˜ç¡®è¯·æ±‚ï¼Œè·³è¿‡ä½œå“ID: {illust_id}")
                        # ä¸ºé¿å…æ— é™é€’å½’ï¼Œä¸åœ¨æ­¤å¤„é€’å½’ï¼Œè€Œæ˜¯æŠ›å‡ºå¼‚å¸¸è®©ç”¨æˆ·é‡è¯•
                        raise Exception("æ£€æµ‹åˆ°ä¸é€‚å½“å†…å®¹ï¼Œå·²è·³è¿‡ã€‚è¯·å°è¯•å…¶ä»–æ ‡ç­¾ã€‚")
                    
                    original_img_url = illust_body["urls"]["original"]
                    regular_img_url = illust_body["urls"]["regular"]
                    
                    # æ„å»ºä»£ç†åçš„å›¾ç‰‡URL
                    proxy_original_url = replace_image_domain(original_img_url)
                    proxy_preview_url = replace_image_domain(regular_img_url)
                    
                    return {
                        "image_url": proxy_original_url,
                        "pid": str(illust_id),
                        "title": illust_body["title"],
                        "author": illust_body["userName"],
                        "author_id": illust_body["userId"],
                        "work_url": f"https://www.pixiv.net/artworks/{illust_id}",
                        "preview_url": proxy_preview_url,
                        "original_url": original_img_url,
                        "stats": {
                            "bookmarks": bookmark_count,
                            "likes": like_count,
                            "views": view_count
                        }
                    }
                
    except Exception as e:
        raise Exception(f"æœç´¢å¤±è´¥: {str(e)}")

def replace_image_domain(url: str) -> str:
    """å°†Pixivå›¾ç‰‡åŸŸåæ›¿æ¢ä¸ºä»£ç†åŸŸåï¼Œå¹¶ç¡®ä¿æ–‡ä»¶æ ¼å¼å…¼å®¹"""
    if not url.startswith("http"):
        url = "https:" + url
    
    proxy_base = PROXY_URL.rstrip('/') + '/'
    
    # ä¿®å¤URLä¸­çš„è½¬ä¹‰å­—ç¬¦
    url = url.replace("%2F", "/").replace("%3A", ":")
    
    if "i.pximg.net" in url:
        url = url.replace("https://i.pximg.net", proxy_base.rstrip('/'))
    elif "pixiv.cat" in url:
        url = url.replace("https://pixiv.cat", proxy_base.rstrip('/'))
    
    # ç¡®ä¿æ–‡ä»¶æ ¼å¼å…¼å®¹ï¼ˆé¿å…WebPç­‰ä¸æ”¯æŒçš„æ ¼å¼ï¼‰
    if url.endswith('.webp'):
        url = url[:-5] + '.jpg'  # è½¬ä¸º jpg
    elif url.endswith('.gif') and 'ugoira' not in url:  # éåŠ¨å›¾GIFè½¬ä¸ºJPG
        url = url[:-4] + '.jpg'
    
    # æ›¿æ¢URLä¸­çš„ç‰¹æ®Šå­—ç¬¦ï¼ˆé˜²æ­¢è·¯å¾„é—®é¢˜ï¼‰
    url = url.replace(' ', '%20').replace('&', '%26').replace('?', '%3F')
    
    return url

# ====== åŸå›¾ä¸“ç”¨å¤„ç†å‡½æ•° ======
async def get_remote_file_size(url: str) -> int:
    """è·å–è¿œç¨‹æ–‡ä»¶å¤§å°ï¼Œé¿å…ä¸‹è½½å¤§æ–‡ä»¶"""
    try:
        proxy = PROXY if USE_PROXY else None
        headers = {
            "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36",
            "Referer": "https://www.pixiv.net/"
        }
        
        async with aiohttp.ClientSession() as session:
            async with session.head(
                url, 
                headers=headers,
                proxy=proxy,
                timeout=aiohttp.ClientTimeout(total=10)
            ) as response:
                if response.status in (200, 206):
                    content_range = response.headers.get('Content-Range', '')
                    if content_range:
                        # ä»Content-Rangeä¸­æå–æ–‡ä»¶å¤§å°ï¼šbytes 0-0/12345678
                        return int(content_range.split('/')[-1])
                    content_length = response.headers.get('Content-Length')
                    if content_length:
                        return int(content_length)
                else:
                    # å°è¯•GETè¯·æ±‚å‰1KB
                    headers['Range'] = 'bytes=0-1023'
                    async with session.get(
                        url,
                        headers=headers,
                        proxy=proxy,
                        timeout=aiohttp.ClientTimeout(total=10)
                    ) as response:
                        if response.status in (200, 206):
                            content_length = response.headers.get('Content-Length')
                            if content_length:
                                # ä¼°ç®—å®Œæ•´æ–‡ä»¶å¤§å°ï¼ˆ1024å­—èŠ‚æ˜¯å¤´éƒ¨ï¼Œæ€»å¤§å°é€šå¸¸å¤§äºå¤´éƒ¨ï¼‰
                                estimated_size = int(content_length)
                                return estimated_size * 10  # ç²—ç•¥ä¼°è®¡
        
        return 0
    except Exception as e:
        logger.warning(f"è·å–æ–‡ä»¶å¤§å°å¤±è´¥: {str(e)}")
        return 0

async def download_original_image(url: str) -> str:
    """å®‰å…¨ä¸‹è½½å¤§æ–‡ä»¶åˆ°ä¸´æ—¶ä½ç½®ï¼Œè¿”å›æ–‡ä»¶è·¯å¾„"""
    file_size = await get_remote_file_size(url)
    if file_size > 50 * 1024 * 1024:  # è¶…è¿‡50MBè­¦å‘Š
        logger.warning(f"âš ï¸ æ£€æµ‹åˆ°è¶…å¤§æ–‡ä»¶ ({file_size/1024/1024:.1f}MB)ï¼Œå¯èƒ½å‘é€å¤±è´¥")
    
    # ç”Ÿæˆå”¯ä¸€æ–‡ä»¶å
    timestamp = int(time.time() * 1000)
    random_str = ''.join(random.choices('abcdefghijklmnopqrstuvwxyz0123456789', k=8))
    ext = os.path.splitext(urllib.parse.urlparse(url).path)[1] or '.jpg'
    
    # ç¡®ä¿æ–‡ä»¶æ‰©å±•åå…¼å®¹
    if ext.lower() in ['.webp', '.avif', '.heic']:
        ext = '.jpg'
    elif ext.lower() == '.svg':
        ext = '.png'
    
    filename = f"pixiv_{timestamp}_{random_str}{ext}"
    temp_path = os.path.join(TEMP_DIR, "pixiv_bot", filename)
    
    logger.info(f"å¼€å§‹ä¸‹è½½åŸå›¾åˆ°: {temp_path} (é¢„ä¼°å¤§å°: {file_size/1024/1024:.2f}MB)")
    
    proxy = PROXY if USE_PROXY else None
    headers = {
        "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36",
        "Referer": "https://www.pixiv.net/"
    }
    
    # åˆ›å»ºSSLä¸Šä¸‹æ–‡ï¼ˆé¿å…SSLéªŒè¯é—®é¢˜ï¼‰
    ssl_context = ssl.create_default_context()
    ssl_context.check_hostname = False
    ssl_context.verify_mode = ssl.CERT_NONE
    
    # é‡è¯•æœºåˆ¶
    for attempt in range(MAX_ATTEMPTS):
        try:
            async with aiohttp.ClientSession() as session:
                async with session.get(
                    url,
                    headers=headers,
                    proxy=proxy,
                    timeout=aiohttp.ClientTimeout(total=DOWNLOAD_TIMEOUT),
                    ssl=ssl_context
                ) as response:
                    if response.status != 200:
                        error_text = await response.text()
                        raise Exception(f"ä¸‹è½½å¤±è´¥ï¼ŒçŠ¶æ€ç : {response.status}, å“åº”: {error_text[:200]}")
                    
                    # åˆ†å—å†™å…¥æ–‡ä»¶ï¼Œé¿å…å†…å­˜æº¢å‡º
                    total_bytes = 0
                    start_time = time.time()
                    
                    async with aiofiles.open(temp_path, 'wb') as f:
                        async for chunk in response.content.iter_chunked(MAX_DOWNLOAD_CHUNK):
                            await f.write(chunk)
                            total_bytes += len(chunk)
                            # æ¯10MBè®°å½•ä¸€æ¬¡è¿›åº¦
                            if total_bytes % (10 * 1024 * 1024) == 0:
                                elapsed = time.time() - start_time
                                speed = total_bytes / elapsed / 1024 / 1024  # MB/s
                                logger.info(f"ä¸‹è½½è¿›åº¦: {total_bytes/1024/1024:.1f}MB, é€Ÿåº¦: {speed:.2f}MB/s")
                    
                    # éªŒè¯æ–‡ä»¶å®Œæ•´æ€§
                    downloaded_size = os.path.getsize(temp_path)
                    if file_size > 0 and downloaded_size < file_size * 0.9:  # å…è®¸10%è¯¯å·®
                        raise Exception(f"æ–‡ä»¶ä¸å®Œæ•´: æœŸæœ› {file_size} å­—èŠ‚, å®é™… {downloaded_size} å­—èŠ‚")
                    
                    # éªŒè¯å›¾ç‰‡æœ‰æ•ˆæ€§ï¼ˆéœ€è¦Pillowï¼‰
                    try:
                        from PIL import Image
                        with Image.open(temp_path) as img:
                            img.verify()  # éªŒè¯æ˜¯å¦ä¸ºæœ‰æ•ˆçš„å›¾ç‰‡æ ¼å¼
                    except ImportError:
                        logger.warning("æœªå®‰è£…Pillowåº“ï¼Œè·³è¿‡å›¾ç‰‡éªŒè¯ã€‚å»ºè®®å®‰è£…: pip install Pillow")
                    except Exception as e:
                        logger.warning(f"å›¾ç‰‡éªŒè¯å¤±è´¥ï¼Œå°è¯•ä¿®å¤: {str(e)}")
                        # å°è¯•ä¿®å¤ï¼šé‡å‘½åæ‰©å±•å
                        if not temp_path.endswith(('.jpg', '.jpeg', '.png')):
                            new_path = temp_path.rsplit('.', 1)[0] + '.jpg'
                            os.rename(temp_path, new_path)
                            temp_path = new_path
                    
                    logger.info(f"âœ… åŸå›¾ä¸‹è½½æˆåŠŸ: {downloaded_size/1024/1024:.2f}MB, è€—æ—¶: {time.time()-start_time:.1f}s")
                    return temp_path
                    
        except Exception as e:
            logger.error(f"ä¸‹è½½å°è¯• {attempt+1}/{MAX_ATTEMPTS} å¤±è´¥: {str(e)}")
            if attempt == MAX_ATTEMPTS - 1:
                raise
            await asyncio.sleep(2)  # é‡è¯•å‰ç­‰å¾…
    
    raise Exception("æ‰€æœ‰ä¸‹è½½å°è¯•å‡å¤±è´¥")

async def cleanup_temp_files():
    """æ¸…ç†24å°æ—¶ä»¥ä¸Šçš„ä¸´æ—¶æ–‡ä»¶"""
    try:
        now = time.time()
        temp_dir = os.path.join(TEMP_DIR, "pixiv_bot")
        
        for filename in os.listdir(temp_dir):
            file_path = os.path.join(temp_dir, filename)
            if os.path.isfile(file_path):
                file_age = now - os.path.getmtime(file_path)
                if file_age > 24 * 3600:  # 24å°æ—¶
                    try:
                        os.remove(file_path)
                        logger.debug(f"æ¸…ç†æ—§ä¸´æ—¶æ–‡ä»¶: {filename}")
                    except Exception as e:
                        logger.warning(f"æ¸…ç†æ–‡ä»¶å¤±è´¥ {filename}: {str(e)}")
    except Exception as e:
        logger.warning(f"æ¸…ç†ä¸´æ—¶æ–‡ä»¶æ—¶å‡ºé”™: {str(e)}")

# ====== Nonebot2æ’ä»¶é€»è¾‘ ======
pixiv_cmd = on_command("pixiv", aliases={"p"}, priority=5, block=True)

@pixiv_cmd.handle()
async def handle_pixiv_command(bot: Bot, event: Event):
    """å¤„ç† /pixiv å‘½ä»¤ - åŸå›¾ä¼˜å…ˆæ¨¡å¼"""
    raw_message = str(event.get_message()).strip()
    command_length = len("/pixiv")
    args = raw_message[command_length:].strip()
    
    if not args:
        await bot.send(event, "è¯·æä¾›æœç´¢æ ‡ç­¾ï¼Œä¾‹å¦‚ï¼š\n/pixiv é¸£æ½®\n/p é¸£æ½®")
        return
    
    tags = [tag.strip() for tag in args.split() if tag.strip()]
    logger.info(f"Pixivæœç´¢è¯·æ±‚: {tags}")
    
    try:
        # 1. æœç´¢ä½œå“
        result = await search_pixiv_by_tag(tags)
        
        # 2. æ„å»ºæ¶ˆæ¯å†…å®¹
        msg_content = (
            f"ğŸ¨ ä½œå“æ ‡é¢˜: {result['title']}\n"
            f"ğŸ‘¤ ä½œè€…: {result['author']} (ID: {result['author_id']})\n"
            f"ğŸ†” ä½œå“ID: {result['pid']}\n"
            f"ğŸ”— ä½œå“é“¾æ¥: {result['work_url']}\n\n"
            f"â³ æ­£åœ¨ä¸‹è½½åŸå›¾ (å¯èƒ½éœ€è¦è¾ƒé•¿æ—¶é—´)..."
        )
        
        # å‘é€åˆæ­¥ä¿¡æ¯
        await bot.send(event, msg_content)
        
        # 3. å®‰å…¨ä¸‹è½½åŸå›¾
        try:
            # æ¸…ç†æ—§ä¸´æ—¶æ–‡ä»¶
            await cleanup_temp_files()
            
            # ä¸‹è½½åŸå›¾
            temp_path = await download_original_image(result['image_url'])
            
            # 4. æ„å»ºæ­£ç¡®çš„CQç  - ä½¿ç”¨fileåè®®
            # ç¡®ä¿è·¯å¾„æ ¼å¼æ­£ç¡®ï¼ˆWindowséœ€è¦ä¸‰ä¸ªæ–œæ ï¼ŒLinux/Macéœ€è¦ä¸¤ä¸ªï¼‰
            if os.name == 'nt':  # Windows
                file_url = f"file:///{temp_path.replace(os.sep, '/')}"
            else:  # Linux/Mac
                file_url = f"file://{temp_path}"
            
            # 5. å‘é€åŸå›¾ - ä½¿ç”¨MessageSegmentç¡®ä¿æ­£ç¡®è§£æ
            start_time = time.time()
            await bot.send(event, MessageSegment.image(file_url))
            logger.info(f"âœ… åŸå›¾å‘é€æˆåŠŸ! è€—æ—¶: {time.time()-start_time:.1f}s")
            
            # 6. å¼‚æ­¥æ¸…ç†æ–‡ä»¶ï¼ˆä¸é˜»å¡å“åº”ï¼‰
            async def delayed_cleanup():
                await asyncio.sleep(30)  # ç­‰å¾…30ç§’ç¡®ä¿å‘é€å®Œæˆ
                try:
                    if os.path.exists(temp_path):
                        os.remove(temp_path)
                        logger.debug(f"å·²æ¸…ç†ä¸´æ—¶æ–‡ä»¶: {temp_path}")
                except Exception as e:
                    logger.warning(f"æ¸…ç†æ–‡ä»¶å¤±è´¥ {temp_path}: {str(e)}")
            
            # åˆ›å»ºåå°ä»»åŠ¡
            asyncio.create_task(delayed_cleanup())
            
        except Exception as e:
            error_msg = str(e)
            logger.error(f"åŸå›¾å‘é€å¤±è´¥: {error_msg}\n{traceback.format_exc()}")
            
            # é™çº§æ–¹æ¡ˆï¼šå‘é€é¢„è§ˆå›¾ + åŸå›¾é“¾æ¥
            fallback_msg = (
                f"âš ï¸ åŸå›¾å‘é€å¤±è´¥ï¼ˆå¯èƒ½æ–‡ä»¶è¿‡å¤§ï¼‰ï¼Œå·²è‡ªåŠ¨é™çº§\n"
                f"ğŸ”— åŸå›¾ä¸‹è½½: {result['image_url']}\n\n"
                f"ğŸ–¼ï¸ å½“å‰æ˜¾ç¤ºé¢„è§ˆå›¾ï¼ˆç‚¹å‡»é“¾æ¥ä¸‹è½½åŸå›¾ï¼‰:"
            )
            await bot.send(event, fallback_msg)
            
            # å‘é€é¢„è§ˆå›¾
            preview_data = await download_and_process_preview(result['preview_url'])
            await bot.send(event, MessageSegment.image(preview_data))
    
    except Exception as e:
        error_msg = str(e)
        logger.error(f"Pixivæœç´¢å¤±è´¥: {error_msg}\n{traceback.format_exc()}")
        
        # ä¼˜åŒ–é”™è¯¯æç¤º
        if "Cookie" in error_msg or "cookie" in error_msg.lower():
            error_msg = (
                "âš ï¸ Cookieæ— æ•ˆï¼è¯·é‡æ–°è·å–Pixiv Cookie:\n"
                "1. ç™»å½• https://www.pixiv.net\n"
                "2. æŒ‰ F12 æ‰“å¼€å¼€å‘è€…å·¥å…·\n"
                "3. è¿›å…¥ Application â†’ Storage â†’ Cookies\n"
                "4. å¤åˆ¶æ•´ä¸ª Cookie å†…å®¹æ›¿æ¢ä»£ç ä¸­çš„ PIXIV_COOKIE"
            )
        elif "ä»£ç†" in error_msg or "proxy" in error_msg.lower() or "Proxy" in error_msg:
            error_msg = (
                "âš ï¸ ä»£ç†é…ç½®é—®é¢˜ï¼è¯·æ£€æŸ¥:\n"
                f"- æœ¬åœ°ä»£ç†: {PROXY}\n"
                f"- Cloudflare ä»£ç†: {PROXY_URL}\n"
                "- ç¡®ä¿ä»£ç†è½¯ä»¶æ­£å¸¸è¿è¡Œ"
            )
        elif "timeout" in error_msg.lower() or "è¶…æ—¶" in error_msg:
            error_msg = (
                "âš ï¸ è¯·æ±‚è¶…æ—¶ï¼å¯èƒ½æ˜¯ç½‘ç»œä¸ç¨³å®šæˆ–ä»£ç†å»¶è¿Ÿè¿‡é«˜\n"
                "å»ºè®®:\n"
                "1. æ£€æŸ¥Clashä»£ç†æ˜¯å¦æ­£å¸¸è¿è¡Œ\n"
                "2. å°è¯•æ›´æ¢æ ‡ç­¾\n"
                "3. æ£€æŸ¥Cloudflare Workersæ˜¯å¦å¯ç”¨"
            )
        elif "memory access out of bounds" in error_msg or "å†…å­˜" in error_msg:
            error_msg = (
                "âš ï¸ å†…å­˜æº¢å‡ºï¼åŸå›¾è¿‡å¤§å¯¼è‡´\n"
                "å·²è‡ªåŠ¨é™çº§å‘é€é¢„è§ˆå›¾\n"
                "æ‚¨ä¹Ÿå¯ä»¥é€šè¿‡ä½œå“é“¾æ¥ä¸‹è½½åŸå›¾"
            )
        elif "404" in error_msg or "403" in error_msg:
            error_msg = (
                "âš ï¸ æ— æ³•è®¿é—®å›¾ç‰‡èµ„æº\n"
                "å¯èƒ½æ˜¯ä»£ç†é…ç½®æœ‰è¯¯æˆ–Pixivé™åˆ¶\n"
                f"åŸå§‹URL: {result.get('original_url', 'æœªçŸ¥') if 'result' in locals() else 'æœªçŸ¥'}"
            )
        else:
            error_msg = f"å‘ç”ŸæœªçŸ¥é”™è¯¯: {error_msg}"
        
        await bot.send(event, f"âŒ æœç´¢å¤±è´¥: {error_msg}")

# ====== é¢„è§ˆå›¾å¤„ç†å‡½æ•°ï¼ˆé™çº§ç”¨ï¼‰ ======
async def download_and_process_preview(image_url: str) -> bytes:
    """ä¸‹è½½å¹¶å¤„ç†é¢„è§ˆå›¾ï¼ˆå°å°ºå¯¸ï¼‰"""
    try:
        proxy = PROXY if USE_PROXY else None
        headers = {
            "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36",
            "Referer": "https://www.pixiv.net/"
        }
        
        async with aiohttp.ClientSession() as session:
            async with session.get(
                image_url,
                headers=headers,
                proxy=proxy,
                timeout=aiohttp.ClientTimeout(total=15)
            ) as response:
                if response.status != 200:
                    raise Exception(f"é¢„è§ˆå›¾ä¸‹è½½å¤±è´¥ï¼ŒçŠ¶æ€ç : {response.status}")
                
                return await response.read()
                
    except Exception as e:
        logger.error(f"é¢„è§ˆå›¾å¤„ç†å¤±è´¥: {str(e)}")
        raise Exception(f"é¢„è§ˆå›¾å¤„ç†å¤±è´¥: {str(e)}")